<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">DETR</h1><p class="page-description">Understanding End-to-End Pipeline of DEtection TRansformer</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-09-30T00:00:00-05:00" itemprop="datePublished">
        Sep 30, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#what-is-detr">What is DETR?</a>
<ul>
<li class="toc-entry toc-h2"><a href="#finds-lets-know-what-is-object-detection">Finds let’s know what is Object Detection</a></li>
<li class="toc-entry toc-h2"><a href="#how-detr-approach-the-problem">How DETR approach the problem?</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#architecture">Architecture</a>
<ul>
<li class="toc-entry toc-h2"><a href="#transformer---attention-is-all-you-need">Transformer - Attention is all you need.</a></li>
<li class="toc-entry toc-h2"><a href="#prediction-through-feed-forward-network">Prediction through Feed Forward Network</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#loss">LOSS</a>
<ul>
<li class="toc-entry toc-h2"><a href="#set-prediction-loss">Set Prediction Loss</a></li>
<li class="toc-entry toc-h2"><a href="#matching-loss">Matching Loss</a>
<ul>
<li class="toc-entry toc-h3"><a href="#bounding-box-loss">Bounding Box Loss</a></li>
<li class="toc-entry toc-h3"><a href="#why-generalized-iou-loss-and-not-just-iou">Why Generalized IOU Loss and not just IOU?</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#auxiliary-decoding-losses">Auxiliary Decoding Losses</a></li>
</ul>
</li>
</ul><h1 id="what-is-detr">
<a class="anchor" href="#what-is-detr" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is DETR?</h1>

<h2 id="finds-lets-know-what-is-object-detection">
<a class="anchor" href="#finds-lets-know-what-is-object-detection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finds let’s know what is Object Detection</h2>

<p>Object detection refers to the capability of computer and software systems to locate objects in an image/scene and identify each object.<br>
Object detection has been widely used for face detection, vehicle detection, pedestrian counting, web images, security systems and driverless cars.</p>

<h2 id="how-detr-approach-the-problem">
<a class="anchor" href="#how-detr-approach-the-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>How DETR approach the problem?</h2>

<p><code class="language-plaintext highlighter-rouge">DETR</code> streamlines the detection pipeline, effectively removing the need for many hand-designed components like a <code class="language-plaintext highlighter-rouge">non-maximum suppression</code> procedure or <code class="language-plaintext highlighter-rouge">anchor generation</code>
that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called <code class="language-plaintext highlighter-rouge">DEtection TRansformer</code> or <code class="language-plaintext highlighter-rouge">DETR</code>, are a <code class="language-plaintext highlighter-rouge">set-based global loss</code> that forces unique predictions via <code class="language-plaintext highlighter-rouge">bipartite matching</code>, and a <code class="language-plaintext highlighter-rouge">transformer encoder-decoder architecture</code>.</p>

<p>Given a fixed small <code class="language-plaintext highlighter-rouge">set of learned object queries</code>, DETR reasons about the relations of the objects and the global image context to directly output
the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors.</p>

<p>DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner</p>

<p>Another helpful aspect unlike most existing detection methods, DETR doesn’t require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes.</p>

<h1 id="architecture">
<a class="anchor" href="#architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Architecture</h1>

<h2 id="transformer---attention-is-all-you-need">
<a class="anchor" href="#transformer---attention-is-all-you-need" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer - Attention is all you need.</h2>

<p><code class="language-plaintext highlighter-rouge">Transformers</code> introduced in 2017, came up with a novel approach of <code class="language-plaintext highlighter-rouge">Self-Attention Mechanism</code> where each element scan through every other element of a sequence and update it by aggregating information from the whole sequence.</p>

<p>The <code class="language-plaintext highlighter-rouge">Transformers</code> changed the entire NLP domain, but wasn’t used until 2020 in the Computer Vision domain by the introduction of <code class="language-plaintext highlighter-rouge">Vision Transformers</code>. (more on it <a href="https://github.com/abdksyed/ViT">here</a>).</p>

<p><img src="../images/ObjectDetection/ViT_model.gif" width="1100" height="800"></p>

<p>DETR uses the same <code class="language-plaintext highlighter-rouge">Self-Attention Mechanism</code> in their <code class="language-plaintext highlighter-rouge">Encoded-Decoder</code> architecture.</p>

<p>In <code class="language-plaintext highlighter-rouge">ViT</code> the images are converted to patches and than flattened and passed through the Linear Projection layer. to get <code class="language-plaintext highlighter-rouge">image embeddings</code> which are added with <code class="language-plaintext highlighter-rouge">positional embeddings</code>. But in DETR, we use a <code class="language-plaintext highlighter-rouge">ResNet50</code> trained on <code class="language-plaintext highlighter-rouge">ImageNet</code> as a Backbone, and the final feature map from ResNet-5 block is taken the shape of the feature map is <code class="language-plaintext highlighter-rouge">d x H/32 x W/32</code> and it is converted to embeddings by <code class="language-plaintext highlighter-rouge">flattening it on H and W</code>, and transposing it to become, <code class="language-plaintext highlighter-rouge">196x256</code> (HW x 256) as Transformers accept such sequential embeddings.</p>

<p>DETR uses <code class="language-plaintext highlighter-rouge">Encoder-Decoder</code> architecture, Each encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network (FFN). Since the transformer architecture is permutation-invariant, it is supplemented with fixed positional encodings that are added to the input of each attention layer. DETR used 6 encoders.</p>

<p>TThe decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed <code class="language-plaintext highlighter-rouge">encoder-decoder attention</code> and <code class="language-plaintext highlighter-rouge">self-attention mechanisms</code>, with the only difference that DETR model decodes the N objects in parallel at each decoder layer, while original Transformer use an autoregressive model that predicts the output sequence one element at a time.</p>

<p>Since the decoder is also permutation-invariant, the N input embeddings must be different to produce different results. These input embeddings(to the decoder) are <code class="language-plaintext highlighter-rouge">learnt positional encodings</code> that are refer to as <strong><code class="language-plaintext highlighter-rouge">object queries</code></strong>, and similarly to the encoder, we add them to the input of each attention layer.<br>
The N object queries are transformed into an output embedding by the decoder. They are then independently decoded into box coordinates and class labels by a feed forward network, resulting N final predictions. Using self and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context.</p>

<h2 id="prediction-through-feed-forward-network">
<a class="anchor" href="#prediction-through-feed-forward-network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prediction through Feed Forward Network</h2>

<p>The final prediction is computed by a <code class="language-plaintext highlighter-rouge">3-layer perceptron</code> with <code class="language-plaintext highlighter-rouge">ReLU</code> activation function and hidden dimension d, and a linear projection layer. The FFN predicts the <code class="language-plaintext highlighter-rouge">normalized center coordinates, height and width of the box w.r.t. the input image</code>, and the linear layer predicts the <code class="language-plaintext highlighter-rouge">class label using a softmax</code> function. Since we predict a
fixed-size set of N bounding boxes, where <em>N is usually much larger than the actual number of objects</em> of interest in an image, an additional <em>special class label ∅</em> is used to represent that no object is detected within a slot. This class plays a similar role to the “background” class in the standard object detection approaches.</p>

<h1 id="loss">
<a class="anchor" href="#loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>LOSS</h1>
<blockquote>
  <p>Let’s see how DETR is trained</p>
</blockquote>

<h2 id="set-prediction-loss">
<a class="anchor" href="#set-prediction-loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Set Prediction Loss</h2>

<p>DETR infers a <em>fixed-size set of N predictions</em>, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image. Since there are large number of predictions, which as said is more than objects present in the image, one of the main difficulties arises in training is to score predicted objects (class, position, size) with respect to the ground truth.<br>
DETR loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses, more specifically the <code class="language-plaintext highlighter-rouge">Hungarian algorithm</code> which is a “combinatorial optimization algorithm that solves the assignment problem in polynomial time”. <a href="https://en.wikipedia.org/wiki/Hungarian_algorithm">ref</a></p>

<p>One of the main benefits of this approach is that it simply produces a set of predictions rather than a list, meaning that the produced boxes are unique (which saves a lot of post-processing time). It also allows them to do box predictions directly rather than doing those predictions with respect to some initial guesses.</p>

<p>The matching is to account for ordering differences in the permutations of the predictions compared to the ground truth. Given a particular loss function ${L}_{match}(\hat{y}, y)$, it finds the permutation for the predictions that gives the minimum total loss. The matching checks the possibilities of all permutations, and selects the one that minimizes the total loss, giving the model the benifit of <em>best possible matching</em> to set of predictions.</p>

<p><img src="../images/ObjectDetection/BipartiteMatching.jpg" alt="BipartiteMatching"></p>

<p>This matching portion plays the same role as heuristic rules used to match proposal or anchors to past ground truth objects in past object detection models. The solution for the above problem is found using the <code class="language-plaintext highlighter-rouge">Hungarain Algorithm</code>. As can be seen from the above image, the name for the loss function comes from the Bipartite Graph that is seen in graph theory.</p>

\[\hat{\sigma} = arg \min\limits_{\sigma \epsilon N} \sum\limits_{i=1}^{N} L_{match}(\hat{y}_i, y_i)\]

<h2 id="matching-loss">
<a class="anchor" href="#matching-loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Matching Loss</h2>

<p>We have seen that the bipartite matching, tries to calculate the matching loss and find the set of matching which gives the least sum of matching loss of each element. Let’s see what is the Matching loss being used here.</p>

<p>${L}<em>{match}(\hat{y}, y)$  is a pair-wise matching cost between ground truth $y</em>{i}$ and a prediction with index $\sigma (i)$. The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes.<br>
Each element i of the ground truth set can be seen as a $y_{i} = (c_{i}, b_{i})$ where $c_{i}$ is the target class label (which may be ∅) and $b_{i}${i}$ ∈ [0, 1] is a vector that defines ground truth box center coordinates and its height and width relative to the image size. 
For the prediction with index $\sigma (i)$ we define probability of class $c_{i}$ as $\hat p_{\sigma(i)}(c_{i})$ and the predicted box as $\hat b_{\sigma(i)}$. With these notations we can define ${L}_{match}(\hat{y}, y)$ as</p>

\[{L}_{match}(\hat{y}, y) = −1_{\{c_{i}=\phi\}} \hat p_{\sigma (i)} (c_{i}) + 1_{\{c_{i}=\phi\}}{L}_{box} (b_{i}, \hat b_{\sigma(i)}).\]

<p>Now, to compute the loss function, the Hungarian loss for all pairs matched in the previous step. We can define the loss similarly to the losses of common object detectors, i.e. a linear combination of a negative log-likelihood for class prediction and a box loss.</p>

\[{L}_{Hungarian}(y, \hat{y}) = \sum\limits_{i=1}^{N} [ -log\hat p_{\sigma (i)} +  1_{\{c_{i}=\phi\}}{L}_{bbox}(b_{i}, \hat b_{\sigma(i)}) ]\]

<p>In actual, the log-probability term when class = $\phi$ is down-weighted by factor of 10 to account for class-imbalance, as there will be very high number of no-objects in the dataset with 100 queries in each image and number of classes being less than that.</p>

<h3 id="bounding-box-loss">
<a class="anchor" href="#bounding-box-loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bounding Box Loss</h3>

<p>The bounding box loss discussed above is a combination of ${L1}$ loss adn the generalized IOU loss. Unlike tradional object detectors which do box prediction based on initial guess like anchors or proposals, DETR make box predictions directly.</p>

<p>\({L}_{bbox}(b_{i}, \hat b_{\sigma(i)}) = \lambda_{iou}{L}_{iou}(b_{i}, \hat b_{\sigma(i)}) + \lambda_{L1}||b_{i}, \hat b_{\sigma(i)}|\)
where $\lambda_{iou} and \lambda_{L1}$ are hyperparameters.</p>

<h3 id="why-generalized-iou-loss-and-not-just-iou">
<a class="anchor" href="#why-generalized-iou-loss-and-not-just-iou" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why Generalized IOU Loss and not just IOU?</h3>

<p>The IOU loss is a loss function that measures the similarity between two boxes. The IOU loss is defined as <code class="language-plaintext highlighter-rouge">intersection</code> over the <code class="language-plaintext highlighter-rouge">union</code> of the two boxes, it has great properties of <code class="language-plaintext highlighter-rouge">scale-invariance</code>  and have been used as a metric in many tasks of detection and segmentation, but it can’t be directly used as an objective function to be optimized.<br>
Also, there is no strong-corelation between minimizing the commonly used losses (like L2 norm) and the IOU values.</p>

<p><img src="../images/ObjectDetection/IoU_Bad.jpg" alt="IoU_Bad"></p>

<p>Let’s assume a case, where we are calculating loss for bouding box, and consider the top-right point being calculated with L1 loss. As we can see above the IoU is varying in each case, but the distance between the top-right point of predicted and actual box is constant, hence giving same L1 Norm values.</p>

<p>Where as the generalized IoU, takes into account the distance between the predicted box and the actual box even if the boxes are not overlapping.</p>

<p><img src="../images/ObjectDetection/GIoU.jpg" alt="GIoU"></p>

<p>The genaralized IoU is defined as <br>
\(GIoU = IoU - \frac{|C-(A \cup B)|}{|C|}\)</p>

<p><img src="../images/ObjectDetection/GIoUvIoU.jpg" alt="GIoUvIoU"></p>

<h2 id="auxiliary-decoding-losses">
<a class="anchor" href="#auxiliary-decoding-losses" aria-hidden="true"><span class="octicon octicon-link"></span></a>Auxiliary Decoding Losses</h2>

<p>It was found helpful to use auxiliary losses in decoder during training, especially to help the model output the correct number of objects of each class. We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. We use an additional shared layer-norm to normalize the input to the prediction FFNs from different decoder layers.</p>


  </div><a class="u-url" href="/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR.html" hidden></a>
</article>