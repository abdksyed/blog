<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">DETR Q/A</h1><p class="page-description">Pondering few Question on DETR</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-09-30T00:00:00-05:00" itemprop="datePublished">
        Sep 30, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#we-take-the-encoded-image-dxh32xw32-and-send-it-to-multi-head-attention-from-where-do-we-take-this-encoded-image">We take the encoded image (dxH/32xW/32) and send it to Multi-Head Attention (FROM WHERE DO WE TAKE THIS ENCODED IMAGE?)</a>
<ul>
<li class="toc-entry toc-h5"><a href="#we-than-along-with-dxn-box-embeddings-send-the-encoded-image-to-the-multi-head-attention">We than along with dxN Box embeddings send the encoded Image to the Multi-Head Attention</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#we-do-something-here-to-generate-n-x-m-x-h32-x-w32-maps-what-do-we-do-here">We do something here to generate N x M x H/32 x W/32 maps. (WHAT DO WE DO HERE?)</a></li>
<li class="toc-entry toc-h3"><a href="#then-we-concatenate-these-maps-with-res5-block-where-is-this-coming-from">Then we concatenate these maps with Res5 Block (WHERE IS THIS COMING FROM?)</a></li>
<li class="toc-entry toc-h3"><a href="#then-we-perform-the-above-steps-explain-these-steps">Then we perform the above steps (EXPLAIN THESE STEPS)</a></li>
<li class="toc-entry toc-h3"><a href="#result">RESULT</a></li>
<li class="toc-entry toc-h3"><a href="#acknowledments">Acknowledments:</a></li>
</ul><p><img src="/images/DETR/0_DETR_Panoptic.png" alt="Panoptic_Flow" title="Panoptic Flow"></p>

<h3 id="we-take-the-encoded-image-dxh32xw32-and-send-it-to-multi-head-attention-from-where-do-we-take-this-encoded-image">
<a class="anchor" href="#we-take-the-encoded-image-dxh32xw32-and-send-it-to-multi-head-attention-from-where-do-we-take-this-encoded-image" aria-hidden="true"><span class="octicon octicon-link"></span></a>We take the encoded image (dxH/32xW/32) and send it to Multi-Head Attention (<strong>FROM WHERE DO WE TAKE THIS ENCODED IMAGE?</strong>)</h3>

<p>The encoded image <em>d x H/32 x W/32</em> is the image which is the output of the <strong><em>transformer encoder</em></strong>. When the final feature map from ResNet-5 block is taken the shape of the feature map is <em>d x H/32 x W/32</em> and it is converted to embeddings by flattening it on H and W, and transposing it to become, 196x256 (HW x 256) as Transformers accept such sequential embeddings. This embeddings after passed through the 6 layer encoder maintains it’s shape, if 196x256, and this final encoded 196x256 is again re-arranged to form the Encoded Image which after completion of object detection is sent to a Multi-head attention layer along with bounding box embeddings.</p>

<p><img src="/images/DETR/7_Encoded_Image.png" alt="Encoder" title="Encoder"></p>

<h5 id="we-than-along-with-dxn-box-embeddings-send-the-encoded-image-to-the-multi-head-attention">
<a class="anchor" href="#we-than-along-with-dxn-box-embeddings-send-the-encoded-image-to-the-multi-head-attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>We than along with dxN Box embeddings send the encoded Image to the Multi-Head Attention</h5>

<h3 id="we-do-something-here-to-generate-n-x-m-x-h32-x-w32-maps-what-do-we-do-here">
<a class="anchor" href="#we-do-something-here-to-generate-n-x-m-x-h32-x-w32-maps-what-do-we-do-here" aria-hidden="true"><span class="octicon octicon-link"></span></a>We do something here to generate N x M x H/32 x W/32 maps. (<strong>WHAT DO WE DO HERE?</strong>)</h3>

<p>We perform a <strong>Multi Head Attention</strong> with the Bounding Box embeddings and the encoded image from the Transformer encoder.</p>

<p>The each box embeddings are dot product with the encoded image, using <em>M</em> attention heads to get the desired <em>N x M x H/32 x W/32</em> attention heatmap, where N are the number of objects detected in the detection pipeline.</p>

<p><img src="/images/DETR/8_Box_Image_attn.png" alt="Box_Image_Attention" title="Box Image Attention"></p>

<h3 id="then-we-concatenate-these-maps-with-res5-block-where-is-this-coming-from">
<a class="anchor" href="#then-we-concatenate-these-maps-with-res5-block-where-is-this-coming-from" aria-hidden="true"><span class="octicon octicon-link"></span></a>Then we concatenate these maps with Res5 Block (<strong>WHERE IS THIS COMING FROM?</strong>)</h3>

<p>After getting the attention maps which are small resolution, H/32 and W/32 needs to be up sampled to get the final segmentation image. To achieve the final prediction and increase the resolution, we use a similar concept of Feature Pyramid Networg</p>

<p>Before all, this during the first step of sending the image through the ResNet-50 backbone, we set aside the feature maps after every ResNet block, i.e. after each block of ResNet the output feature maps are saved in separate place, to be used here in the FPN</p>

<ul>
  <li>Res Block 2 -&gt; H/4 x W/4</li>
  <li>Res Block 3 -&gt; H/8 x W/8</li>
  <li>Res Block 4 -&gt; H/16 x W/16</li>
  <li>Res Block 5 -&gt; H/32 x W/32</li>
</ul>

<p>FPN-style Network:</p>

<p>​	The attention maps are send through a convolutional network, where at first the attention maps are concatenated with the respective size Res Block, i.e., in the beginning the Attention map is of size H/32 x W/32, so the Res5 block which have the feature maps of same size are concatenated.</p>

<p>Hence the Res5/Res4/Res3/Res2 feature maps are coming from the Backbone CNN which was used initially to generate the image embeddings, where each feature map was saved and set aside after every block.</p>

<p><img src="/images/DETR/12_Res_Middle.png" alt="ResNet_FeatureMaps" title="ResNet FeatureMaps"></p>

<h3 id="then-we-perform-the-above-steps-explain-these-steps">
<a class="anchor" href="#then-we-perform-the-above-steps-explain-these-steps" aria-hidden="true"><span class="octicon octicon-link"></span></a>Then we perform the above steps (<strong>EXPLAIN THESE STEPS</strong>)</h3>

<p>As mentioned above, the attention maps after Multi-Head Attention is concatenated with Res5 block feature maps and is send through a two set of Conv-Norm-Activation layer and than upsampled to become H/16 x W/16. Again the similar process is repeated where corresponding feature maps from ResNet blocks are added and send though Conv-Norm-Activation Layers, and finally a Conv-Norm-Activation-Conv layer is added to get the final attention maps. Here although we use the HxW image, the final attention maps are of the size H/4 x W/4, since in the beginning of ResNet it self we downsample the image by 4 times, hence that’s the final Mask Logits are 4 times smaller.</p>

<p>The Convolutions are all 3x3 Kernels, and the Normalization being used is Group Normalization and ReLU activation is performed after every Conv-Norm.</p>

<p><img src="/images/DETR/11_Res_Up.png" alt="Maps_2_Masks" title="Maps to Masks"></p>

<h3 id="result">
<a class="anchor" href="#result" aria-hidden="true"><span class="octicon octicon-link"></span></a>RESULT</h3>

<p>This than is passed though Pixel-wise Argmax and concatenated to get the final Panoptic Segmentation Mask.</p>

<p><img src="/images/DETR/10_Panoptic.png" alt="Panoptic_Result" title="Panoptic Results"></p>

<h3 id="acknowledments">
<a class="anchor" href="#acknowledments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Acknowledments:</h3>
<p>All Drawings done on <a href="https://app.diagrams.net/">draw.io</a><br>
DETR Paper: <a href="https://arxiv.org/abs/2005.12872">arxiv</a><br>
Excellent Demo from Author: <a href="https://www.youtube.com/watch?v=utxbUlo9CyY">Video</a><br>
Great Explanation on Paper: <a href="https://www.youtube.com/watch?v=T35ba_VXkMY">Yanic</a>, <a href="https://www.youtube.com/watch?v=BNx-wno-0-g">AI Epiphany</a></p>

  </div><a class="u-url" href="/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR-QA.html" hidden></a>
</article>