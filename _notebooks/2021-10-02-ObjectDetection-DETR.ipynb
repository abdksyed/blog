{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DEâ«¶TR -- End-to-End Object Detection with Transformers\n",
    "> An End to End pipeline for Object Detection.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Custom Dataset, Object Detection, Panoptic Segmentation, COCO ,DETR]\n",
    "- image: images/CustomDataset/Predicted.png "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Cool](../images/ObjectDetection/Cover.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code has been forked from this [awesome video tutorial](https://www.youtube.com/watch?v=RkhXoj_Vvr4), and the here is the [git repo](https://github.com/thedeepreader/detr_tutorial)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Dataset Script for DETR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# detr/datasets/construction.py\n",
    "class ConstructionDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
    "        super(ConstructionDetection, self).__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super(ConstructionDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "        return img, target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our dataset class `ConstructionDetection` is inherited from `torchvision.datasets.CocoDetection`, and it does all the weight lifting of calling images and the labels. We than apply our transformations if any to the images, and serve the tuple of images and labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# detr/datasets/construction.py\n",
    "\n",
    "import datasets.transforms as T\n",
    "\n",
    "normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "if image_set == 'train':\n",
    "    return T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomSelect(\n",
    "            T.RandomResize(scales, max_size=1333),\n",
    "            T.Compose([\n",
    "                T.RandomResize([400, 500, 600]),\n",
    "                T.RandomSizeCrop(384, 600),\n",
    "                T.RandomResize(scales, max_size=1333),\n",
    "            ])\n",
    "        ),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "if image_set == 'val':\n",
    "    return T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        normalize,\n",
    "    ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "DETR uses ImageNet standard-deviation and mean for the normalization.\n",
    "\n",
    "For training dataloader, the transformations have `Random Horizontal Flip` and than it randomly selects between a `Random Resize` or collection of `Random Resize`, `Random Size Crop` and again `Random Resize`. The random resizing takes place at various scales of *[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]*\n",
    "\n",
    "Whereas for validation, the images are resized to width of 800, with the height of max 1333 and than normalized."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model and Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine Tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# detr/main.py\n",
    "\n",
    "if args.resume:\n",
    "        if args.resume.startswith('https'): # If argument is link\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "            del checkpoint['model']['class_embed.weight']\n",
    "            del checkpoint['model']['class_embed.bias']\n",
    "            strict_ = False # To allow model to load without class_embed if num classes is different\n",
    "        else: # if argument is .pth file\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "            strict_ = True # Since we use our own pth, the num classes remain same, so strict loading"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We fine-tune(transfer learning) the `DETR-r50` model which is trained on COCO dataset, to learn on our Custom dataset.\n",
    "\n",
    "The model which is trained on COCO has 91 classes, and the num_classes for DETR is 92 here, as DETR also have no-object class. But whereas our dataset only has 63 classes, we can reduce the final layer to 63 from 92. We could have also kept it to 92, as it wouldn't effect the result apart from slight increase in number of parameters as mentioned by the authors. As the weights won't take part in the prediction and would be just dead weights.\n",
    "\n",
    "As we will see in the next part of Panoptic Segmentation, where the authors use 250 as number of classes, since it doesn't effect. Only thing to take care is num_classes must be atleast one greater than actual number of class.\n",
    "\n",
    "> Important: If our class ids are not continous for example if we have 3 classe with id `1`,`32` and `94`, than we will have to keep the `num_classes` as 95. The no-object class will be one greater than the `max_id` of the classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# collapse-hide\n",
    "\n",
    "if args.dataset_file == \"construction\":\n",
    "    num_classes = 63+1 # 63 Classes + 1 for no object\n",
    "if args.dataset_file == \"construction_panoptic\":\n",
    "    # for panoptic, we just add a num_classes that is large enough to hold\n",
    "    # max_obj_id + 1, but the exact value doesn't really matter\n",
    "    num_classes = 63+1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We had marked classes from 0 to 47 for `things` and from 48-63 for `stuff`, so for us the max_id for the classes is 63, hence we can give num_classes as `63+1`, where the 1 class at end is for `no-object`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we can run the training from this command.\n",
    "\n",
    "!python main.py --dataset_file construction --data_path ./datasets \\\n",
    "        --device cuda --output_dir /content/output --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth \\\n",
    "        --epochs <number_of_epochs> --batch_size <batch_size>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we see, we can pass link to `resume` argument, which is the pre trained weight of COCO, which is used to start the training from, and after each epoch, the weights of the epoch is saved in the `output` folder.\n",
    "\n",
    "So, we can continue training from the last epoch, by passing the checkpoint weights path to the `resume` argument, and the training will start from the last epoch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference and Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# detr/test.py\n",
    "\n",
    "orig_image = Image.open(img_sample)\n",
    "transform = make_coco_transforms(\"val\") # Resize to 800 and normalize\n",
    "\n",
    "outputs = model(image)\n",
    "\n",
    "outputs[\"pred_logits\"] = outputs[\"pred_logits\"].cpu() # Get Prediction Scores\n",
    "outputs[\"pred_boxes\"] = outputs[\"pred_boxes\"].cpu() # Get Predtiction Bounding Boxes\n",
    "\n",
    "# keep = probas.max(-1).values > 0.85\n",
    "keep = probas.max(-1).values > args.thresh\n",
    "\n",
    "# Rescale the predictions from 0,1 to image size\n",
    "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], orig_image.size) \n",
    "\n",
    "plot_bbox(image, bboxes_scaled)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mAP (mean Average Precision)\n",
    "\n",
    "In computer vision, mAP is a popular evaluation metric used for object detection.\n",
    "\n",
    "Before understanding let's see what is Precision and Recall\n",
    "\n",
    "`Precision` measures how accurate your predictions are. i.e. the percentage of your predictions are correct. It measures how many of the predictions that your model made were actually correct.\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "`Recall` measures how well you find all the positives i.e. for all the correct ground truth, the percetange of your predictions are correct.\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "We can see that the numerator for both Precision and Recall has `True Positives`, but the denominator changes for each. Both are equally important and depens on the application, and many times it's used in together by using harmonic mean for `F1-score` and so on.\n",
    "\n",
    "> INFO: Excellent explanation on Precision and Recall with great examples - [YouTube Link](youtube.com/watch?v=O4joFUqvz40)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In classification, getting True Positives, False Positives and False Negatives, is straight forward, where if the ground truth is cat, and the model predicted is cat, than it's a True Postive, where as if the model predicts cat and it's not a cat, which indicated False Positive, and if the model predicts not cat and it's a cat, which indicated False Negative.  \n",
    "\n",
    "In simpler words, its combination of did model predict correct (True/False), what is the class (Postive/Negative).\n",
    "\n",
    "But how do we get all these in Object Detection, where the model not only finds the class but also the Bounding Box, and for that let's take a small de tour to understand IoU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IoU (Intersection over Union)\n",
    "\n",
    "For each bounding box, we measure an overlap between the predicted bounding box and the ground truth bounding box. This is measured by `IoU`(intersection over union).  \n",
    "Which is the `intersection` of the ground truth box with the predicted box, divided by the `union` of the ground truth box with the predicted box.\n",
    "\n",
    "$$\n",
    "IoU = \\frac{Area of intersection}{Area of union}\n",
    "$$\n",
    "\n",
    "![IoU](../images/ObjectDetection/IoU.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, since we know about IoU, in Object Detection, for True Positive, we assign a `threshold` to the IoU, and if the IoU is greater than the threshold, then it's a True Positive and if not, then it's a False Positive. \n",
    "\n",
    "Let's say we have a ground truth bounding box of `[0,0,100,100]` and the model predicts a bounding box of `[50,50,100,100]`, then the IoU is `0.5`, which is less than the threshold of 0.5, so it's a False Positive when thresshold is 0.5. But the same predicted box will be a True Positive if the IoU is less than 0.5 say 0.3."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Average Precision (or even mean Average Precision) is the average over all categories of the precision at particular IoU threshold, and is denoted as `mAP@0.x`, where `0.x` is the IoU threshold.\n",
    "\n",
    "We also have soemthing as `mAP@0.5:0.95:0.05` which is the mean over all APs ranging from 0.5 to 0.95 at every step of 0.05."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mean Average Recall\n",
    "\n",
    "Similarly, Average Recall is same as mAP with the use of Recall instead of precision"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Cool Sample Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Sample1](../images/ObjectDetection/sample1.png)\n",
    "\n",
    "\n",
    "![Sample2](../images/ObjectDetection/sample2.png)\n",
    "\n",
    "\n",
    "![Sample3](../images/ObjectDetection/sample3.png)\n",
    "\n",
    "\n",
    "![Sample4](../images/ObjectDetection/sample4.png)\n",
    "\n",
    "\n",
    "![Sample5](../images/ObjectDetection/sample5.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('deep': conda)"
  },
  "interpreter": {
   "hash": "09ecada1698966b9942b894559f43d01c7dd9fca749532a1ab4ca6a2b150e647"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}