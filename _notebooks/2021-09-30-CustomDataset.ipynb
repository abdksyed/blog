{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Custom Dataset using DETR\n",
    "> An approach for creating Dataset for Panoptic Segmentation using Pre Trained DETR.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Custom Dataset, Object Detection, Panoptic Segmentation, COCO ,DETR]\n",
    "<!-- - image: images/chart-preview.png -->"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What and Why?\n",
    "\n",
    "We at [TSAI](https://theschoolof.ai/) wanted to train a Panpotic Segmentation Model(DETR) on a custom dataset. We decided to go ahead with Construction Classes and annotated the things of around 50 classes like [grader](https://www.google.com/search?q=grader), [wheel loader](https://www.google.com/search?q=grader), [aac blocks](https://www.google.com/search?q=aac+blocks) etc.  \n",
    "While annotating we just annoatated the things, and left out stuff, as it would be way more tedious to annotate the stuffs like ground, grass, building etc. \n",
    "\n",
    "We know existing Models are very well trained on COCO dataset, and we could levrage them to predict stuff classes in our images. So we went out and decided to use pre-trained DETR for Panoptic Segmentation to perform inference on our images and get this stuffs for our images.\n",
    "\n",
    "In total we had **10k Images** for all the classes combined, with very high imbalance, like 15 images for one class and on the other extreme 500+ images for other.."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Classes for Things and Stuff"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Things\n",
    "\n",
    "We have 48 Things categories."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "{\n",
    "    'aac_blocks': 0, 'adhesives': 1, 'ahus': 2, 'aluminium_frames_for_false_ceiling': 3,\n",
    "    'chiller': 4, 'concrete_mixer_machine': 5, 'concrete_pump_(50%)': 6, 'control_panel': 7,\n",
    "    'cu_piping': 8, 'distribution_transformer': 9, 'dump_truck___tipper_truck': 10,\n",
    "    'emulsion_paint': 11, 'enamel_paint': 12, 'fine_aggregate': 13, 'fire_buckets': 14,\n",
    "    'fire_extinguishers': 15, 'glass_wool': 16, 'grader': 17, 'hoist': 18,\n",
    "    'hollow_concrete_blocks': 19, 'hot_mix_plant': 20, 'hydra_crane': 21,\n",
    "    'interlocked_switched_socket': 22, 'junction_box': 23, 'lime': 24, 'marble': 25,\n",
    "    'metal_primer': 26, 'pipe_fittings': 27, 'rcc_hume_pipes': 28, 'refrigerant_gas': 29,\n",
    "    'river_sand': 30, 'rmc_batching_plant': 31, 'rmu_units': 32, 'sanitary_fixtures': 33,\n",
    "    'skid_steer_loader_(bobcat)': 34, 'smoke_detectors': 35, 'split_units': 36,\n",
    "    'structural_steel_-_channel': 37, 'switch_boards_and_switches': 38, 'texture_paint': 39,\n",
    "    'threaded_rod': 40, 'transit_mixer': 41, 'vcb_panel': 42, 'vitrified_tiles': 43,\n",
    "    'vrf_units': 44, 'water_tank': 45, 'wheel_loader': 46, 'wood_primer': 47\n",
    "}\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stuff\n",
    "\n",
    "To make life simpler, I decided to make the stuff categories smaller, by collapsing all the categories to their super categories, and finally leavins us with 16 stuff classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "{\n",
    "    'building':48, 'ceiling':49, 'floor':50, 'food':51, 'furniture':52,\n",
    "    'ground':53, 'plant':54, 'raw_material':55, 'sky':56, 'solids':57,\n",
    "    'structural':58, 'textile':59, 'wall':60, 'water':61, 'window':62,\n",
    "    'thing':63\n",
    "}\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mapping for each stuff category of COCO to their super category: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<details>\n",
    "  <summary>Categories to Super Categories</summary>\n",
    "\n",
    "  ```\n",
    "      {\n",
    "        'cardboard': 'raw_material', 'paper': 'raw_material', 'plastic': 'raw_material',  \n",
    "        'metal': 'raw_material', \n",
    "\n",
    "        'wall-tile': 'wall', 'wall-panel': 'wall',\n",
    "        'wall-wood': 'wall', 'wall-brick': 'wall', 'wall-stone': 'wall',\n",
    "        'wall-concrete': 'wall', 'wall-other': 'wall', \n",
    "\n",
    "        'ceiling-tile': 'ceiling', 'ceiling-other': 'ceiling', \n",
    "        \n",
    "        'carpet': 'floor', 'floor-tile': 'floor', 'floor-wood': 'floor',\n",
    "        'floor-marble': 'floor', 'floor-stone': 'floor', 'floor-other': 'floor', \n",
    "\n",
    "        'window-blind': 'window', 'window-other': 'window',\n",
    "\n",
    "        'door-stuff': 'furniture', 'desk-stuff': 'furniture', 'table': 'furniture',\n",
    "        'shelf': 'furniture', 'cabinet': 'furniture', 'cupboard': 'furniture',\n",
    "        'mirror-stuff': 'furniture', 'counter': 'furniture', 'light': 'furniture',\n",
    "        'stairs': 'furniture', 'furniture-other': 'furniture', \n",
    "\n",
    "        'rug': 'textile', 'mat': 'textile', 'towel': 'textile', 'napkin': 'textile',\n",
    "        'clothes': 'textile', 'cloth': 'textile', 'curtain': 'textile',\n",
    "        'blanket': 'textile', 'pillow': 'textile', 'banner': 'textile','textile-other': 'textile', \n",
    "\n",
    "        'fruit': 'food', 'salad': 'food', 'vegetable': 'food', 'food-other': 'food', \n",
    "\n",
    "        'house': 'building', 'skyscraper': 'building','bridge': 'building', \n",
    "        'tent': 'building', 'roof': 'building', 'building-other': 'building',\n",
    "\n",
    "        'fence': 'structural', 'cage': 'structural', 'net': 'structural', 'railing': 'structural', \n",
    "        'structural-other': 'structural',\n",
    "\n",
    "        'grass': 'plant', 'tree': 'plant', 'bush': 'plant', 'leaves': 'plant',\n",
    "        'flower': 'plant', 'branch': 'plant', 'moss': 'plant', 'straw': 'plant',\n",
    "        'plant-other': 'plant',\n",
    "\n",
    "        'clouds': 'sky', 'sky-other': 'sky',\n",
    "\n",
    "        'wood': 'solids', 'rock': 'solids', 'stone': 'solids', \n",
    "        'mountain': 'solids', 'hill': 'solids', 'solid-other': 'solids',\n",
    "\n",
    "        'sand': 'ground', 'snow': 'ground', 'dirt': 'ground', 'mud': 'ground',\n",
    "        'gravel': 'ground', 'road': 'ground', 'pavement': 'ground','railroad': 'ground',\n",
    "        'platform': 'ground', 'playingfield': 'ground', 'ground-other': 'ground',\n",
    "\n",
    "        'fog': 'water', 'river': 'water', 'sea': 'water', 'waterdrops': 'water',\n",
    "        'water-other': 'water',\n",
    "        \n",
    "        'things': 'things', 'water': 'water', 'window': 'window', 'ceiling': 'ceiling',\n",
    "        'sky': 'sky', 'floor': 'floor', 'food': 'food', 'building': 'building','wall': 'wall'\n",
    "      }\n",
    "  ```\n",
    "  \n",
    "</details>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Annotations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Actual Image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "I = Image.open(<image_dir>/'images'/img['file_name']) # Sample Image\n",
    "I = I.convert('RGB')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "![ActualImage](../images/CustomDataset/Actual.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Class Annotation (Segmentation and BBox)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# get all images containing given categories, select one at random\n",
    "catIds = coco.getCatIds(catNms=['grader']); # Sample Category\n",
    "imgIds = coco.getImgIds(catIds=catIds ); # Get Image Ids of all images containing the given category\n",
    "img = coco.loadImgs(imgIds[np.random.randint(0,len(imgIds))])[0] #Random Image\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None) # Get annotation ids of all annotations for the img\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ActualAnnotation](../images/CustomDataset/ActualAnnotation.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "og_poly = []\n",
    "for ann in anns: # For each annotation\n",
    "    poly = np.array(ann['segmentation'][0]).reshape((int(len(ann['segmentation'][0])/2), 2)) # Create Array from segmentation\n",
    "    poly = Polygon(poly) # Convert to matplotlib Polygon\n",
    "    og_poly.append(poly)\n",
    "\n",
    "class_mask = np.zeros((og_w,og_h))\n",
    "for op in og_poly:\n",
    "    cv2.fillPoly(class_mask, pts = np.int32([op.get_xy()]), color =(255)) # Paste our Annotations\n",
    "plt.imshow(class_mask) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![AnnotationMask](../images/CustomDataset/AnnotationMask.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "!git clone -q https://github.com/facebookresearch/detr.git #Facebook DETR\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"detr/\"))\n",
    "# Load DETR trained on COCO for Panoptic Segmentation with ResNet101.\n",
    "model, postprocessor = torch.hub.load('detr', 'detr_resnet101_panoptic', source='local', pretrained=True, return_postprocessor=True, num_classes=250)\n",
    "model.eval()\n",
    "print('Loaded!')\n",
    "\n",
    "img = transform(I).unsqueeze(0) #Resize to 800 Width and Normalize\n",
    "out = model(img) # Model Output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention Maps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# compute the scores, excluding the \"no-object\" class (the last one)\n",
    "scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
    "# threshold the confidence\n",
    "keep = scores > 0.85\n",
    "\n",
    "# Plot all the remaining masks\n",
    "ncols = 2\n",
    "fig, axs = plt.subplots(ncols=ncols, nrows=math.ceil(keep.sum().item() / ncols), figsize=(18, 10))\n",
    "\n",
    "mask_log_list = []\n",
    "for i, (attn_map,logit) in enumerate(zip(out[\"pred_masks\"][keep], out[\"pred_logits\"][keep])):\n",
    "    logit = logit.softmax(-1).argmax().item()\n",
    "    if logit > 92: # If stuff of COCO\n",
    "        det_id = meta.stuff_dataset_id_to_contiguous_id[logit]\n",
    "        logit = meta.stuff_classes[det_id]\n",
    "    mask_log_list.append((attn_map,logit))\n",
    "    axs.ravel()[i].imshow(attn_map, cmap=\"cividis\")\n",
    "    axs.ravel()[i].axis('off')\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![AttentionMaps_civ](../images/CustomDataset/AttentionMaps_civ.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the model nicely predicts masks for each class. The predictions of the models are `car`, `truck`, `sand`, `sky`, `person` and `tree`.  \n",
    "The class maps are pretty darn good."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DETR Post-Processed Mask"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
    "result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]\n",
    "\n",
    "# We extract the segments info and the panoptic result from DETR's prediction\n",
    "segments_info = deepcopy(result[\"segments_info\"])\n",
    "\n",
    "# Panoptic predictions are stored in a special format png\n",
    "panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n",
    "print(panoptic_seg.size)\n",
    "final_w, final_h = panoptic_seg.size\n",
    "\n",
    "# We convert the png into an segment id map\n",
    "panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8)\n",
    "panoptic_seg = torch.from_numpy(rgb2id(panoptic_seg))\n",
    "\n",
    "# Detectron2 uses a different numbering of coco classes, here we convert the class ids accordingly\n",
    "meta = MetadataCatalog.get(\"coco_2017_val_panoptic_separated\")\n",
    "for i in range(len(segments_info)):\n",
    "    c = segments_info[i][\"category_id\"]\n",
    "    segments_info[i][\"category_id\"] = meta.thing_dataset_id_to_contiguous_id[c] if segments_info[i][\"isthing\"] else meta.stuff_dataset_id_to_contiguous_id[c]\n",
    "\n",
    "\n",
    "# Finally we visualize the prediction\n",
    "v = Visualizer(numpy.array(I.copy().resize((final_w, final_h)))[:, :, ::-1], meta, scale=1.0)\n",
    "v._default_font_size = 20\n",
    "v = v.draw_panoptic_seg_predictions(panoptic_seg, segments_info, area_threshold=0)\n",
    "cv2_imshow(v.get_image())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Predicted](../images/CustomDataset/Predicted.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Warning: Woahhhhhhhh, this is Bad!\n",
    "\n",
    "The `car`, `tree`, `sand`, `sky` and `person` came out nicely. But the `truck` is pretty bad as the back area has got leaked into the right region.  \n",
    "If we look at the above attention maps now, we see that the region between `tree` and `sand` is not identified, the DETR post-processor spreads the masks of nearby class to regions where there are no predictions above the given threshold."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even after pasting our `grader` annotation, there will be the `truck` annotation marked pixels in the image and the `sand` which is leaked.\n",
    "\n",
    "![Predicted_OnlyMask_AnnotationOnTop](../images/CustomDataset/Predicted_OnlyMask_AnnotationOnTop.png) \n",
    "\n",
    "And also the border of our annotations may still have the class as `truck`, so this masks will cause a problem, when we train our DETR.  \n",
    "DETR identifies objects using the edges, as shown in their example. The `truck` masks may case an issue, where our model may predict our `grader` as both `grader` and also `truck`.\n",
    "![DETRExample](../images/CustomDataset/DETRExample.png)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention Mask ArgMax Maps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# Taking only Stuff into Consideration\n",
    "# Things of COCO is Void(id=0) for us\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "palette = itertools.cycle(sns.color_palette())\n",
    "color_list = {}\n",
    "combined_attn = np.zeros(out['pred_masks'].shape[2:] + (3,))\n",
    "for attn_map, logit, class_id in mask_log_list:\n",
    "    color = (np.asarray(next(palette)) * 255)\n",
    "    color_list[class_id] = color\n",
    "    combined_attn[attn_map>0] = color\n",
    "    combined_attn = combined_attn.astype(np.int)\n",
    "\n",
    "plt.imshow(combined_attn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead, we can directly ArgMax on the attention maps of the stuff classes, ignoring the things classes of COCO, since we wouldn't want the same issue as mentioned above, and also avoiding the leakage of class by not using the inbuilt DETR post-processor.\n",
    "\n",
    "Here the regions which are not predicted, will be marked with black pixels, which in [COCO Dataset](https://cocodataset.org/#home) is `void` class\n",
    "\n",
    "![CombinedAttentionMask_AnnotationOnTop](../images/CustomDataset/CombinedAttentionMask_AnnotationOnTop.png)\n",
    "\n",
    "The mask with the BBox(class_id,area).\n",
    "\n",
    "![Predicted_MaskBBox_AnnotationOnTop](../images/CustomDataset/Predicted_MaskBBox_AnnotationOnTop.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final JSON for All Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "# Create Annotations of Each Class\n",
    "\n",
    "def run_class(class_images_anns, class_name):\n",
    "    class_annotaion = []\n",
    "    class_image = []\n",
    "\n",
    "\n",
    "    class_color = CATEGORY_COLOR[class_name] # Get Color Tuple for the class\n",
    "    for img in tqdm(class_images_anns): # For each annotation in the JSON\n",
    "\n",
    "        # The Image annotations has .jpg whereas actual Image is png and vice-versa.\n",
    "        # try and except to get correct image accordingly\n",
    "        try:\n",
    "            I = Image.open(dataDir/class_name/'images'/img['file_name'])\n",
    "        except FileNotFoundError:\n",
    "            if img['file_name'].endswith('.jpg'):\n",
    "                I = Image.open(dataDir/class_name/'images'/img['file_name'].replace('jpg','png'))\n",
    "            elif img['file_name'].endswith('.png'):\n",
    "                I = Image.open(dataDir/class_name/'images'/img['file_name'].replace('png','jpg'))\n",
    "\n",
    "        # Convert any grayscale or RBGA to RGB\n",
    "        I = I.convert('RGB')\n",
    "        og_h, og_w = I.size\n",
    "\n",
    "        # Get Annotation of the Image\n",
    "        annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "\n",
    "        # Create Polygon for custom Annotation.\n",
    "        og_poly = gen_original_poly(anns)\n",
    "\n",
    "        # Get DETR output on our Image w.r.t COCO classes\n",
    "        trans_img = transform(I).unsqueeze(0)\n",
    "        out = model(trans_img.to('cuda'))\n",
    "\n",
    "        # Create Masks by stacking Attention Maps and Pasting our Annotation\n",
    "        # Excluding the functions definition for brevity. Can be found from colab link.\n",
    "        class_masks = generate_class_maps(out)\n",
    "        pred_mask, color2class = generate_pred_masks(class_masks, out['pred_masks'].shape[2:])\n",
    "\n",
    "        pred_mask = cv2.resize(pred_mask, (og_h, og_w), interpolation= cv2.INTER_NEAREST)\n",
    "        #Pasting Our Class on Mask\n",
    "        for op in og_poly:\n",
    "            cv2.fillPoly(pred_mask, pts = np.int32([op.get_xy()]), color = class_color)\n",
    "        \n",
    "        #Convering Mask to ID using panopticapi.utils\n",
    "        mask_id = rgb2id(pred_mask)\n",
    "        \n",
    "        # Final Segmentation Details\n",
    "        segments_info = generate_gt(mask_id, color2class, class_name)\n",
    "        \n",
    "        # The ID image(1 Channel) converted to 3 Channel Mask to save.\n",
    "        img_save = Image.fromarray(id2rgb(mask_id))\n",
    "        mask_file_name = img['file_name'].split('.')[0] + '.png'\n",
    "        img_save.save(dataDir/class_name/'annotations'/mask_file_name)\n",
    "\n",
    "\n",
    "        # Appending the Image Annotation to List\n",
    "        class_annotaion.append(\n",
    "            {\n",
    "            \"segments_info\": segments_info,\n",
    "            \"file_name\": mask_file_name,\n",
    "            \"image_id\":  int(img['id'])\n",
    "            }\n",
    "            )\n",
    "        \n",
    "    return class_annotaion, class_image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "for class_name in list_class: # Loop over all the classes names\n",
    "\n",
    "    annFile = <images_dir>/class_name/'coco.json' # Path to the annotations file of each class\n",
    "    coco = COCO(annFile) # Convert JSON to coco object (pycocotools.COCO)\n",
    "    cats = coco.loadCats(coco.getCatIds())\n",
    "\n",
    "    # get all images containing given categories, select one at random\n",
    "    catIds = coco.getCatIds(catNms=[class_name]);\n",
    "    imgIds = coco.getImgIds(catIds=catIds);\n",
    "    images = coco.loadImgs(imgIds)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(<images_dir>/class_name/'annotations') # Create Annotations Folder for each Class\n",
    "    except FileExistsError as e:\n",
    "        print('WARNING!', e)\n",
    "\n",
    "    CLASS_ANNOTATION = run_class(images, class_name) # Generate Annotations for each class\n",
    "    \n",
    "    FINAL_JSON = {}\n",
    "\n",
    "    FINAL_JSON['licenses'] = coco.dataset['licenses']\n",
    "    FINAL_JSON['info'] = coco.dataset['info']\n",
    "    FINAL_JSON['categories'] = CATEGORIES\n",
    "    FINAL_JSON['images'] = coco.dataset['images']\n",
    "    FINAL_JSON['annotations'] = CLASS_ANNOTATION\n",
    "\n",
    "    out_file = open(<images_dir>/class_name/'annotations'/f'{class_name}.json', \"w\")\n",
    "    json.dump(FINAL_JSON, out_file, indent = 4)\n",
    "    out_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## COCO Format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features with 80 object(things) classes, and 91 stuff classes for several tasks like captioning, segmentation, detection etc.\n",
    "\n",
    "And it is the most widely used data as well as most widely used data format. So converting our annotations to the COCO format, would help us in levraging pre-built tools to create data pipelines to model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### File Structure\n",
    "\n",
    "```\n",
    "<dataset_dir>/\n",
    "    data/\n",
    "        <filename0>.<ext>\n",
    "        <filename1>.<ext>\n",
    "        ...\n",
    "\n",
    "    annotations/\n",
    "        <filename0>.png\n",
    "        <filename1>.png\n",
    "        ...\n",
    "        \n",
    "    labels.json\n",
    "```\n",
    "\n",
    "The `data` folder has all the images, the images can be in image formats like JPG, JPEG, PNG.  \n",
    "The `annotations` folder has the images of the masks, for every image in the `data` folder, with the same name and `.png` extension. The stem name of the file should match with the image in the `data`.  \n",
    "The `labels.json` has 5 main keys, `info`, `licenses`, `categories`, `images` and `annotations`. This json file holds the data for the ground truth of the images. The format of the JSON can be varying as per the problem like object-detection, segmentation, keypoint-detection or image-captioning.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"info\": info, \n",
    "    \"licenses\": [license],\n",
    "    \"categories\": [categories]\n",
    "    \"images\": [image], \n",
    "    \"annotations\": [annotation], \n",
    "}\n",
    "```\n",
    "\n",
    "The `images`, `info` and `licenses` remains same for all types, where as the `annotations` and `categories` format will differ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "info = {\n",
    "  \"year\": int,\n",
    "  \"version\": str,\n",
    "  \"description\": str,\n",
    "  \"contributor\": str,\n",
    "  \"url\": str,\n",
    "  \"date_created\": datetime,\n",
    "        }\n",
    "\n",
    "image = {\n",
    "    \"id\": int,\n",
    "    \"width\": int,\n",
    "    \"height\": int,\n",
    "    \"file_name\": str,\n",
    "    \"license\": int,\n",
    "    \"flickr_url\": str,\n",
    "    \"coco_url\": str,\n",
    "    \"date_captured\": datetime,\n",
    "}\n",
    "\n",
    "license = {\n",
    "    \"id\": int,\n",
    "    \"name\": str,\n",
    "    \"url\": str,\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Object Detection\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each object instance annotation contains a series of fields, including the category id and segmentation mask(optional if only Detection) of the object.  \n",
    "An enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed). Finally, the categories field of the annotation structure stores the mapping of category id to category and supercategory names\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "annotation = {\n",
    "    \"id\": int, \"image_id\": int,\n",
    "    \"category_id\": int,\n",
    "    \"segmentation\": RLE or [polygon],\n",
    "    \"area\": float,\n",
    "    \"bbox\": [x,y,width,height],\n",
    "    \"iscrowd\": 0 or 1,\n",
    "  }\n",
    "\n",
    "categories = [{\n",
    "    \"id\": int, \"name\": str, \"supercategory\": str,\n",
    "    }]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Panoptic Segmentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the panoptic task, each annotation struct is a per-image annotation rather than a per-object annotation. Each per-image annotation has two parts: (1) a PNG that stores the class-agnostic image segmentation and (2) a JSON struct that stores the semantic information for each image segment\n",
    "\n",
    "* To match an annotation with an image, use the image_id field (that is annotation.image_id==image.id).\n",
    "* For each annotation, per-pixel segment ids are stored as a single PNG as the same name as image.\n",
    "* Each segment (whether it's a stuff or thing segment) is assigned a unique id.\n",
    "* Unlabeled pixels (void) are assigned a value of 0. Note that when you load the PNG as an RGB image, you will need to compute the ids via ids=R+G\\*256+B\\*256^2.\n",
    "* In annotation file. The segment_info.id stores the unique id of the segment and is used to retrieve the corresponding mask from the PNG (ids==segment_info.id).\n",
    "* Finally, each category struct has two additional fields: isthing that distinguishes stuff and thing categories and color that is useful for consistent visualization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "annotation = {\n",
    "    \"image_id\": int,\n",
    "    \"file_name\": str,\n",
    "    \"segments_info\": [segment_info],\n",
    "}\n",
    "\n",
    "segment_info = {\n",
    "    \"id\": int,\n",
    "    \"category_id\": int,\n",
    "    \"area\": int,\n",
    "    \"bbox\": [x,y,width,height],\n",
    "    \"iscrowd\": 0 or 1,\n",
    "}\n",
    "\n",
    "categories = [{\n",
    "    \"id\": int,\n",
    "    \"name\": str,\n",
    "    \"supercategory\": str,\n",
    "    \"isthing\": 0 or 1,\n",
    "    \"color\": [R,G,B],\n",
    "}]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}