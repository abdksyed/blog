{
  
    
        "post0": {
            "title": "Python Name Mangling",
            "content": "class Person(object): def __init__(self, name, age): self.name = name self._age = age if age &gt;= 0 else 0 self.________a_ = 10 @property def age(self): return self._age # @age.setter # def age(self, age): # if age &gt;= 0: # self._age = age # else: # self._age = 0 . dir(a) . [&#39;_Person________a_&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_age&#39;, &#39;age&#39;, &#39;name&#39;] . a = Person(&#39;Bob&#39;, 20) . a.age . 20 . a.age = 21 . AttributeError Traceback (most recent call last) /tmp/ipykernel_789/926215696.py in &lt;module&gt; -&gt; 1 a.age = 21 AttributeError: can&#39;t set attribute .",
            "url": "https://abdksyed.github.io/blog/jupyter/2021/12/03/NameMangling.html",
            "relUrl": "/jupyter/2021/12/03/NameMangling.html",
            "date": " ‚Ä¢ Dec 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Learning JavaScript - Basics",
            "content": "Basics of JavaScript . Setup . Downloading NodeJS on WSL2 . Install cURL (a tool used for downloading content from the internet in the command-line) with: sudo apt-get install curl . | Install nvm, with: curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash . | To verify installation, enter: command -v nvm ‚Ä¶this should return ‚Äònvm‚Äô, if you receive ‚Äòcommand not found‚Äô or no response at all, close your current terminal, reopen it, and try again. Learn more in the nvm github repo . | Install the latest stable LTS release of Node.js (recommended): nvm install --lts . | List what versions of Node are installed: nvm ls . | Verify that Node.js is installed and the currently default version with: node --version. Then verify that you have npm as well, with: npm --version (You can also use which node or which npm to see the path used for the default versions). . | . Hello Javascript . console.log(&#39;Hello Javascript&#39;); &gt; Hello Javascript . var someVariable = &#39;Hello Javascript&#39;; console.log(someVariable); &gt; Hello Javascript . Variables . Java Script 3 major data types: . number | string | boolean | . var fullName = &#39;ThisIsMyName&#39;; console.log(fullName, typeOf fullName); &gt; ThisIsMyName string var amICorrect = true; console.log(amICorrect, typeOf amICorrect); &gt; true boolean var myNumber = 47; console.log(myNumber, typeOf myNumber); &gt; 47 number . The variables which are defined using var are given undefined as value, until they are assigned a value. This is done in the Memory Creation phase. Which we will be coming across in the future posts. . var noVar; console.log(noVar); &gt;&gt;&gt; undefined noVar = &quot;I&#39;m not a variable&quot;; console.log(noVar); &gt;&gt;&gt; I&#39;m not a variable . We can take input from the user, for example in the browser we can use the prompt function to create a pop-up box, where the user can enter a value. . var takeInput; takeInput = prompt(&quot;Give me some Input&quot;); . For printing the varialbes in between string data and which is a multi-line we can use the interpolation. Notice how we are using backtick symbol to denote the start of the string and the backtick symbol again to denote the end of the string. And to have any varialbe we can use ${variable_name}. . console.log(` This is a multi-line string. Where I can even call variables. like ${fullName} and ${myNumber}. `) &gt;&gt;&gt; This is a multi-line string. Where I can even call variables. like ThisIsMyName and 47. . Operators . We can use +, -, * and / to perform arithmetic operations. . var listPrice = 799 var salePrice = 199 var discount = ( (listPrice - salePrice)/listPrice ) * 100; console.log(`${discount}%`); &gt;&gt;&gt; 75.09386733416771% . The asnwer which has so many decimal points can be made clean by using round functin from Math. . displayDiscount = Math.round(discount); console.log(`${displayDiscount}%`); &gt;&gt;&gt; 75% . Operator Precedence can be best understand from here . Conditionals . if (&#39; &#39;) console.log(&#39;This is True&#39;); &gt;&gt;&gt; This is True if (1) console.log(&#39;This is True&#39;); &gt;&gt;&gt; This is True if (&#39;FALSE&#39;) console.log(&#39;This is True&#39;); &gt;&gt;&gt; This is True if (true) console.log(&#39;This is True&#39;); &gt;&gt;&gt; This is True if (false) console.log(&#39;This is NOT going to be Printed&#39;); &gt;&gt;&gt; if (0) console.log(&#39;This is NOT going to be Printed&#39;); &gt;&gt;&gt; if (&#39;&#39;) console.log(&#39;This is NOT going to be Printed&#39;); &gt;&gt;&gt; if (null) console.log(&#39;This is NOT going to be Printed&#39;); &gt;&gt;&gt; if (0) console.log(&#39;This is NOT going to be Printed&#39;); &gt;&gt;&gt; var iAmGoingToBeUndefined; if (iAmGoingToBeUndefined) console.log(&#39;This is NOT going to be Printed&#39;); &gt;&gt;&gt; var someArray = []; // new Array() if (someArray) console.log(&#39;This is True&#39;); &gt;&gt;&gt; This is True var someObject = {}; // new Object() if (someObject) console.log(&#39;This is True&#39;); &gt;&gt;&gt; This is True . The if condition will be evaluated and run the code inside the block if the condition is true. So to run the code of if block the condition must be Truthy. . In JavaScript, 0, false, &#39;&#39;, null, NaN, undefined are Falsy values. Remaining everything else is Truthy. . Nested ifs and Logical Operators. . var isEmployee = true; var isLoggenIn = true; var isAdmin = true; if (isEmployee) { if (isLoggenIn) { if (isAdmin) { console.log(&#39;Welcome Mr. Adminva&#39;); } } } &gt;&gt;&gt; Welcome Mr. Adminva . The content will be printed only if all the three are true. But there is a better approach using Logical AND (&amp;&amp;) operator. . if (isEmployee &amp;&amp; isLoggenIn &amp;&amp; isAdmin) { console.log(&#39;Welcome Mr. Adminva&#39;); } else { console.log(&#39;You are not authorized&#39;); } &gt;&gt;&gt; Welcome Mr. Adminva . Similary if we want to pass the condition if any of the condition is true, then we use Logical OR(||) operator. . var isMom = false; var isDad = false; var isSister = true; if (isMom || isDad || isSister) { console.log(&#39;You are a family member&#39;); } else { console.log(&#39;Kaun hai miya tum??r&#39;); } &gt;&gt;&gt; You are a family member . Ternery Operator . The ternery operator ? whatever is before, if evaluated to true first expression is executed, otherwise the second expression is executed. . Syntax: condition ? expression1 : expression2 . var authenticated = true; if (authenticated) { console.log(&#39;Welcome&#39;); } else { console.log(&#39;Show login Options&#39;); } &gt;&gt;&gt; Welcome // Alternate shorted approach console.log(authenticated ? &#39;Welcome&#39; : &#39;Show login Options&#39;); &gt;&gt;&gt; Welcome . Switch and Case . The switch statement is used to perform different actions based on different conditions. . Given a value, the switch statement evaluates the value and executes the code block(case) corresponding to the value. . var userType = &#39;Analyst&#39;; switch (userType) { case &#39;Manager&#39;: console.log(&#39;Welcome Manager&#39;); break; case &#39;Admin&#39;: console.log(&#39;Welcome Admin&#39;); break; case &#39;Developer&#39;: console.log(&#39;Welcome Developer&#39;); break; case &#39;Tester&#39;: console.log(&#39;Welcome Tester&#39;); break; case &#39;Analyst&#39;: console.log(&#39;Welcome Analyst&#39;); break; default: console.log(&#39;Welcome&#39;); } &gt;&gt;&gt; Welcome Analyst . If we look carefully, we have a break after each case, because we want to stop the execution of the code block after the case is executed. If we don‚Äôt have the break statement, the code block will be executed for all the cases which are the below the current case. . Type Coercion . console.log(&quot;2&quot; + 2); &gt;&gt;&gt; 22 . ü§Øü§Øü§Øü§Ø Whattttttttt‚Ä¶ . Java Scripts does type coercion which is convert one data type to another automatically when it faces any issue with the statement. Like in above example adding a string and a number is not possible, so JS automatically converts the number to string and concatenates the two. . var user = 2; if (user == &quot;2&quot;){ console.log(&#39;Condition should be false, but........&#39;); } &gt;&gt;&gt; Condition should be false, but........ . As discussed, here JavaScript does type coercion and convertes user which is number 2 to string ‚Äú2‚Äù. So the condition is true. . We can avoid this by using === operator. . var user = 2; if (user === &quot;2&quot;){ console.log(&#39;Condition should be false, but........&#39;); } &gt;&gt;&gt; . END! . That is it for today, getting to know some basics in Java Script. We will see more in the upcoming days. . Note: This material is created as part of learning Java Script from various resource, like Hitesh Choudhary‚Äôs JS Course and Akshay Saini‚Äôs Namaste JS Course .",
            "url": "https://abdksyed.github.io/blog/javascript/beginner/2021/12/02/Intro-JS.html",
            "relUrl": "/javascript/beginner/2021/12/02/Intro-JS.html",
            "date": " ‚Ä¢ Dec 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "DE‚´∂TR -- Extending Object Detection to Panoptic Segmentation",
            "content": ". Creating Dataset class for Panoptic Segmentation . class ConstructionPanoptic: def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True): with open(ann_file, &#39;r&#39;) as f: self.coco = json.load(f) # Readig the json file # sort &#39;images&#39; field so that they are aligned with &#39;annotations&#39; # i.e., in alphabetical order self.coco[&#39;images&#39;] = sorted(self.coco[&#39;images&#39;], key=lambda x: x[&#39;id&#39;]) self.coco[&#39;annotations&#39;] = sorted(self.coco[&#39;annotations&#39;], key=lambda x: x[&#39;image_id&#39;]) # sanity check, image names in images is same as in annotations masks. if &quot;annotations&quot; in self.coco: for img, ann in zip(self.coco[&#39;images&#39;], self.coco[&#39;annotations&#39;]): #print(img[&#39;file_name&#39;], ann[&#39;file_name&#39;]) assert img[&#39;file_name&#39;].split(&#39;.&#39;)[:-1] == ann[&#39;file_name&#39;].split(&#39;.&#39;)[:-1] self.img_folder = img_folder self.ann_folder = ann_folder self.ann_file = ann_file self.transforms = transforms self.return_masks = return_masks def __getitem__(self, idx): ann_info = self.coco[&#39;annotations&#39;][idx] if &quot;annotations&quot; in self.coco else self.coco[&#39;images&#39;][idx] img_ext = Path(self.coco[&#39;images&#39;][idx][&#39;file_name&#39;]).suffix img_path = Path(self.img_folder) / ann_info[&#39;file_name&#39;].replace(&#39;.png&#39;, img_ext) ann_path = Path(self.ann_folder) / ann_info[&#39;file_name&#39;] img = Image.open(img_path).convert(&#39;RGB&#39;) w, h = img.size if &quot;segments_info&quot; in ann_info: masks = np.asarray(Image.open(ann_path), dtype=np.uint32) # Read the mask file masks = rgb2id(masks) # Convert the mask to the id format ids = np.array([ann[&#39;id&#39;] for ann in ann_info[&#39;segments_info&#39;]]) # Get the unique ids of classes in mask masks = masks == ids[:, None, None] masks = torch.as_tensor(masks, dtype=torch.uint8) labels = torch.tensor([ann[&#39;category_id&#39;] for ann in ann_info[&#39;segments_info&#39;]], dtype=torch.int64) target = {} target[&#39;image_id&#39;] = torch.tensor([ann_info[&#39;image_id&#39;] if &quot;image_id&quot; in ann_info else ann_info[&quot;id&quot;]]) if self.return_masks: target[&#39;masks&#39;] = masks target[&#39;labels&#39;] = labels # Calculating BBox using mask, we already have BBox in annotations, we could have directly call ann_info[&#39;bbox&#39;] target[&quot;boxes&quot;] = masks_to_boxes(masks) # target[&quot;boxes&quot;] = ann_info[&#39;bbox&#39;] target[&#39;size&#39;] = torch.as_tensor([int(h), int(w)]) target[&#39;orig_size&#39;] = torch.as_tensor([int(h), int(w)]) if &quot;segments_info&quot; in ann_info: for name in [&#39;iscrowd&#39;, &#39;area&#39;]: target[name] = torch.tensor([ann[name] for ann in ann_info[&#39;segments_info&#39;]]) if self.transforms is not None: img, target = self.transforms(img, target) return img, target def __len__(self): return len(self.coco[&#39;images&#39;]) def get_height_and_width(self, idx): img_info = self.coco[&#39;images&#39;][idx] height = img_info[&#39;height&#39;] width = img_info[&#39;width&#39;] return height, width . . For panoptic segmentation, we would want the dataset to return the image and the segmentation mask when we call the __getitem__ method. We also return additoinal details like class label, bounding boxes, orignal size of the image as part of target along with the mask itself. . Also, if there are any transformations given, we apply those transformations on both the image and the mask. . As discussed in the Creating Custom Dataset for DETR post, the mask of the corresponding image should have the same name as the image with the suffix .png in the annotations folder. . Transformations . # detr/datasets/construction.py import datasets.transforms as T normalize = T.Compose([ T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800] if image_set == &#39;train&#39;: return T.Compose([ T.RandomHorizontalFlip(), T.RandomSelect( T.RandomResize(scales, max_size=1333), T.Compose([ T.RandomResize([400, 500, 600]), T.RandomSizeCrop(384, 600), T.RandomResize(scales, max_size=1333), ]) ), normalize, ]) if image_set == &#39;val&#39;: return T.Compose([ T.RandomResize([800], max_size=1333), normalize, ]) . . DETR uses ImageNet standard-deviation and mean for the normalization. . For training dataloader, the transformations have Random Horizontal Flip and than it randomly selects between a Random Resize or collection of Random Resize, Random Size Crop and again Random Resize. The random resizing takes place at various scales of [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800] . Whereas for validation, the images are resized to width of 800, with the height of max 1333 and than normalized. . Model . The DETR model for the Panoptic Segmentation, remains same as the DETR model for the Object Detection, with an addition of mask head after the decoder part. Let&#39;s explore the model pipeline, how we use backbone(ResNet-50) generated features at every stage in the mask head. . A standard CNN is used to extract a compact feature representation of the image. Here a pre-trained ResNet50 model was used, and the features after the 5th block were extracted where the size of the image is compacted to H. W but with 2048 channels. Where H is H0/32, W is W0/32 and H0 &amp; W0 are the initial Height and Width of the Image. A Conv2d is used to bring down the channels size from 2048 to 256 . . While the forward pass of the ResNet, the activation maps after each block of Res2(H0/4 x W0/4 x 128), Res3(H0/8 x W0/8x 256), Res4(H0/16 x W0/16 x 512) and Res5(H0/32 x W0/32 x 1028) are saved and set aside for the images which are to be used in the Panoptic segmentation down the line. . Sending to Transformer . The resultant compact features are now sent to the Transformer encoder-decoder architecture. But since Transformers expect sequential inputs, the compact features of size HxWx256 is flattened out to be HWx256 (in PyTorch, the tensor is 256xHxW, so after flattening to 256xHW, it is also transposed to HWx256) . In comparison to ViT where the Image is converted 196x768 embedding for 224x224 image, here the embedding we get is 196x256 (as after Res5 block, we have map size of 14x14 as HxW, so flattening it out gives 196 and we have 256 channels, giving embedding of 196x256). . As we discuees in the DETR part, Transformers are permutation equivariant, which in simpler terms means that the Transformers are not aware of the 2D structure of the image, permuting the inputs just permutes the outputs, and have no effect. So we will have to make sure to add some positional awareness to the inputs. . And this can be done in several ways, one way to do is adding learnable parameters to the input embeddings, where the networks will learn the positional encodings and the other way, as done in original Transformer, is adding some kind of fixed positional embedding like one-hot encoding or using some type of mathematical functions like sin functions with respect to the input patch position to generate the positional embedding. In DETR with empirical results the showed that the sin encodings had marginal better results than learnt embeddings, so these were used in the process. . . After we get the Image embeddings, we can send to a Transformer Encoder, here we have 6 layers of encoders, where output of one encoder is sent as input to other, to increase the model capacity and improve training. . Since the transformers, work on sequence and maintain the sequence length, the input is a sequence of image patches and the output of all the encoders are also patches which can be again converted to form/shape of an image, so here after the image is encoded throught the encoder, we save the encoded image separately for further usage(which we will see soon in mask head part), and the sequence is sent to the decoder . . The sequence as discussed is sent to the decoder, but as a key and value, where as the query for the atetntion mechanis, comes from the Object Queries. The object queries here are fixed number of sequence of length N (here DETR used 100), where are initially initialized randomly, and the object queries intuitively works as a set entities asking the image(embeddings from encoder) regions about the presence of objects. . If we see above, taken from the paper, they visualized 20 object queries, where each of the query, is getting information of presence of object at particular area, for differnet sizes of object, which is illustrated as three colors of green/red/purple. We can see that each object query has different area t concentrate for differnt object sizes. . Than we after the decoder layer, passes the final sequence from decoder to the Feed Forward Netwrok, to get the class_label and the bounding box of the object for each object, and since we added no object, now the model can predict 100 objects in the image, and if there are less than the model predicts as no-objects for those object queries . . Mask Head . After the Bounding Box detection, now we add our mask head, which is used part of End-to-End model, to get Panoptic segmentation of the image. . This is acheived by doing attention of the obejct queries of the classes on the encoded image which we saved and set aside after the encoder transformer, and each object query after going through attention on the encoded image, will result in a mask for the class of the object query. . . After getting these maps, which are of size H/32 x W/32, we would want to upscale the image and we now use an upsampling model, and also add the feature maps which we saved after every layer during the first step of ResNet forward pass. . . And finally we can do Pixel wise Arg max to achieve the final segmentation map. . . Some Cool Segmentations . . . . .",
            "url": "https://abdksyed.github.io/blog/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/03/PanopticSegmentation-DETR.html",
            "relUrl": "/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/03/PanopticSegmentation-DETR.html",
            "date": " ‚Ä¢ Oct 3, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "DE‚´∂TR -- End-to-End Object Detection with Transformers",
            "content": ". The code has been forked from this awesome video tutorial, and the here is the git repo . Creating Dataset Script for DETR . # detr/datasets/construction.py class ConstructionDetection(torchvision.datasets.CocoDetection): def __init__(self, img_folder, ann_file, transforms, return_masks): super(ConstructionDetection, self).__init__(img_folder, ann_file) self._transforms = transforms self.prepare = ConvertCocoPolysToMask(return_masks) def __getitem__(self, idx): img, target = super(ConstructionDetection, self).__getitem__(idx) image_id = self.ids[idx] target = {&#39;image_id&#39;: image_id, &#39;annotations&#39;: target} img, target = self.prepare(img, target) if self._transforms is not None: img, target = self._transforms(img, target) return img, target . . Our dataset class ConstructionDetection is inherited from torchvision.datasets.CocoDetection, and it does all the weight lifting of calling images and the labels. We than apply our transformations if any to the images, and serve the tuple of images and labels. . Transformations . # detr/datasets/construction.py import datasets.transforms as T normalize = T.Compose([ T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800] if image_set == &#39;train&#39;: return T.Compose([ T.RandomHorizontalFlip(), T.RandomSelect( T.RandomResize(scales, max_size=1333), T.Compose([ T.RandomResize([400, 500, 600]), T.RandomSizeCrop(384, 600), T.RandomResize(scales, max_size=1333), ]) ), normalize, ]) if image_set == &#39;val&#39;: return T.Compose([ T.RandomResize([800], max_size=1333), normalize, ]) . . DETR uses ImageNet standard-deviation and mean for the normalization. . For training dataloader, the transformations have Random Horizontal Flip and than it randomly selects between a Random Resize or collection of Random Resize, Random Size Crop and again Random Resize. The random resizing takes place at various scales of [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800] . Whereas for validation, the images are resized to width of 800, with the height of max 1333 and than normalized. . Model and Training . Fine Tuning . # detr/main.py if args.resume: if args.resume.startswith(&#39;https&#39;): # If argument is link checkpoint = torch.hub.load_state_dict_from_url( args.resume, map_location=&#39;cpu&#39;, check_hash=True) del checkpoint[&#39;model&#39;][&#39;class_embed.weight&#39;] del checkpoint[&#39;model&#39;][&#39;class_embed.bias&#39;] strict_ = False # To allow model to load without class_embed if num classes is different else: # if argument is .pth file checkpoint = torch.load(args.resume, map_location=&#39;cpu&#39;) strict_ = True # Since we use our own pth, the num classes remain same, so strict loading . . We fine-tune(transfer learning) the DETR-r50 model which is trained on COCO dataset, to learn on our Custom dataset. . The model which is trained on COCO has 91 classes, and the num_classes for DETR is 92 here, as DETR also have no-object class. But whereas our dataset only has 63 classes, we can reduce the final layer to 63 from 92. We could have also kept it to 92, as it wouldn&#39;t effect the result apart from slight increase in number of parameters as mentioned by the authors. As the weights won&#39;t take part in the prediction and would be just dead weights. . As we will see in the next part of Panoptic Segmentation, where the authors use 250 as number of classes, since it doesn&#39;t effect. Only thing to take care is num_classes must be atleast one greater than actual number of class. . Important: If our class ids are not continous for example if we have 3 classe with id 1,32 and 94, than we will have to keep the num_classes as 95. The no-object class will be one greater than the max_id of the classes. . if args.dataset_file == &quot;construction&quot;: num_classes = 63+1 # 63 Classes + 1 for no object if args.dataset_file == &quot;construction_panoptic&quot;: # for panoptic, we just add a num_classes that is large enough to hold # max_obj_id + 1, but the exact value doesn&#39;t really matter num_classes = 63+1 . . We had marked classes from 0 to 47 for things and from 48-63 for stuff, so for us the max_id for the classes is 63, hence we can give num_classes as 63+1, where the 1 class at end is for no-object . !python main.py --dataset_file construction --data_path ./datasets --device cuda --output_dir /content/output --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --epochs &lt;number_of_epochs&gt; --batch_size &lt;batch_size&gt; . If we see, we can pass link to resume argument, which is the pre trained weight of COCO, which is used to start the training from, and after each epoch, the weights of the epoch is saved in the output folder. . So, we can continue training from the last epoch, by passing the checkpoint weights path to the resume argument, and the training will start from the last epoch. . Inference and Metrics . # detr/test.py orig_image = Image.open(img_sample) transform = make_coco_transforms(&quot;val&quot;) # Resize to 800 and normalize outputs = model(image) outputs[&quot;pred_logits&quot;] = outputs[&quot;pred_logits&quot;].cpu() # Get Prediction Scores outputs[&quot;pred_boxes&quot;] = outputs[&quot;pred_boxes&quot;].cpu() # Get Predtiction Bounding Boxes # keep = probas.max(-1).values &gt; 0.85 keep = probas.max(-1).values &gt; args.thresh # Rescale the predictions from 0,1 to image size bboxes_scaled = rescale_bboxes(outputs[&#39;pred_boxes&#39;][0, keep], orig_image.size) plot_bbox(image, bboxes_scaled) . . Metrics . mAP (mean Average Precision) . In computer vision, mAP is a popular evaluation metric used for object detection. . Before understanding let&#39;s see what is Precision and Recall . Precision measures how accurate your predictions are. i.e. the percentage of your predictions are correct. It measures how many of the predictions that your model made were actually correct. . $$ Precision = frac{TP}{TP+FP} $$Recall measures how well you find all the positives i.e. for all the correct ground truth, the percetange of your predictions are correct. . $$ Recall = frac{TP}{TP+FN} $$We can see that the numerator for both Precision and Recall has True Positives, but the denominator changes for each. Both are equally important and depens on the application, and many times it&#39;s used in together by using harmonic mean for F1-score and so on. . INFO:Excellent explanation on Precision and Recall with great examples - YouTube Link . In classification, getting True Positives, False Positives and False Negatives, is straight forward, where if the ground truth is cat, and the model predicted is cat, than it&#39;s a True Postive, where as if the model predicts cat and it&#39;s not a cat, which indicated False Positive, and if the model predicts not cat and it&#39;s a cat, which indicated False Negative. . In simpler words, its combination of did model predict correct (True/False), what is the class (Postive/Negative). . But how do we get all these in Object Detection, where the model not only finds the class but also the Bounding Box, and for that let&#39;s take a small de tour to understand IoU. . IoU (Intersection over Union) . For each bounding box, we measure an overlap between the predicted bounding box and the ground truth bounding box. This is measured by IoU(intersection over union). Which is the intersection of the ground truth box with the predicted box, divided by the union of the ground truth box with the predicted box. . $$ IoU = frac{Area of intersection}{Area of union} $$ . Now, since we know about IoU, in Object Detection, for True Positive, we assign a threshold to the IoU, and if the IoU is greater than the threshold, then it&#39;s a True Positive and if not, then it&#39;s a False Positive. . Let&#39;s say we have a ground truth bounding box of [0,0,100,100] and the model predicts a bounding box of [50,50,100,100], then the IoU is 0.5, which is less than the threshold of 0.5, so it&#39;s a False Positive when thresshold is 0.5. But the same predicted box will be a True Positive if the IoU is less than 0.5 say 0.3. . Average Precision (or even mean Average Precision) is the average over all categories of the precision at particular IoU threshold, and is denoted as mAP@0.x, where 0.x is the IoU threshold. . We also have soemthing as mAP@0.5:0.95:0.05 which is the mean over all APs ranging from 0.5 to 0.95 at every step of 0.05. . mean Average Recall . Similarly, Average Recall is same as mAP with the use of Recall instead of precision . Some Cool Sample Results . . . . . .",
            "url": "https://abdksyed.github.io/blog/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/02/ObjectDetection-DETR.html",
            "relUrl": "/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/02/ObjectDetection-DETR.html",
            "date": " ‚Ä¢ Oct 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "DETR",
            "content": "What is DETR? . Finds let‚Äôs know what is Object Detection . Object detection refers to the capability of computer and software systems to locate objects in an image/scene and identify each object. Object detection has been widely used for face detection, vehicle detection, pedestrian counting, web images, security systems and driverless cars. . How DETR approach the problem? . DETR streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. . Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. . DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner . Another helpful aspect unlike most existing detection methods, DETR doesn‚Äôt require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes. . Architecture . Transformer - Attention is all you need. . Transformers introduced in 2017, came up with a novel approach of Self-Attention Mechanism where each element scan through every other element of a sequence and update it by aggregating information from the whole sequence. . The Transformers changed the entire NLP domain, but wasn‚Äôt used until 2020 in the Computer Vision domain by the introduction of Vision Transformers. (more on it here). . . DETR uses the same Self-Attention Mechanism in their Encoded-Decoder architecture. . In ViT the images are converted to patches and than flattened and passed through the Linear Projection layer. to get image embeddings which are added with positional embeddings. But in DETR, we use a ResNet50 trained on ImageNet as a Backbone, and the final feature map from ResNet-5 block is taken the shape of the feature map is d x H/32 x W/32 and it is converted to embeddings by flattening it on H and W, and transposing it to become, 196x256 (HW x 256) as Transformers accept such sequential embeddings. . DETR uses Encoder-Decoder architecture, Each encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network (FFN). Since the transformer architecture is permutation-invariant, it is supplemented with fixed positional encodings that are added to the input of each attention layer. DETR used 6 encoders. . TThe decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed encoder-decoder attention and self-attention mechanisms, with the only difference that DETR model decodes the N objects in parallel at each decoder layer, while original Transformer use an autoregressive model that predicts the output sequence one element at a time. . Since the decoder is also permutation-invariant, the N input embeddings must be different to produce different results. These input embeddings(to the decoder) are learnt positional encodings that are refer to as object queries, and similarly to the encoder, we add them to the input of each attention layer. The N object queries are transformed into an output embedding by the decoder. They are then independently decoded into box coordinates and class labels by a feed forward network, resulting N final predictions. Using self and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context. . . Prediction through Feed Forward Network . The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer. The FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image, and the linear layer predicts the class label using a softmax function. Since we predict a fixed-size set of N bounding boxes, where N is usually much larger than the actual number of objects of interest in an image, an additional special class label ‚àÖ is used to represent that no object is detected within a slot. This class plays a similar role to the ‚Äúbackground‚Äù class in the standard object detection approaches. . LOSS . Let‚Äôs see how DETR is trained . Set Prediction Loss . DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image. Since there are large number of predictions, which as said is more than objects present in the image, one of the main difficulties arises in training is to score predicted objects (class, position, size) with respect to the ground truth. DETR loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses, more specifically the Hungarian algorithm which is a ‚Äúcombinatorial optimization algorithm that solves the assignment problem in polynomial time‚Äù. ref . One of the main benefits of this approach is that it simply produces a set of predictions rather than a list, meaning that the produced boxes are unique (which saves a lot of post-processing time). It also allows them to do box predictions directly rather than doing those predictions with respect to some initial guesses. . The matching is to account for ordering differences in the permutations of the predictions compared to the ground truth. Given a particular loss function ${L}_{match}( hat{y}, y)$, it finds the permutation for the predictions that gives the minimum total loss. The matching checks the possibilities of all permutations, and selects the one that minimizes the total loss, giving the model the benifit of best possible matching to set of predictions. . . This matching portion plays the same role as heuristic rules used to match proposal or anchors to past ground truth objects in past object detection models. The solution for the above problem is found using the Hungarain Algorithm. As can be seen from the above image, the name for the loss function comes from the Bipartite Graph that is seen in graph theory. . $$ hat{ sigma} = arg min limits_{ sigma epsilon N} sum limits_{i=1}^{N} L_{match}( hat{y}_i, y_i) $$ Matching Loss . We have seen that the bipartite matching, tries to calculate the matching loss and find the set of matching which gives the least sum of matching loss of each element. Let‚Äôs see what is the Matching loss being used here. . ${L}_{match}( hat{y}, y)$ is a pair-wise matching cost between ground truth $y_{i}$ and a prediction with index $ sigma (i)$. The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. Each element i of the ground truth set can be seen as a $y_{i} = (c_{i}, b_{i})$ where $c_{i}$ is the target class label (which may be ‚àÖ) and $b_{i}$ ‚àà [0, 1] is a vector that defines ground truth box center coordinates and its height and width relative to the image size. For the prediction with index $ sigma (i)$ we define probability of class $c_{i}$ as $ hat p_{ sigma(i)}(c_{i})$ and the predicted box as $ hat b_{ sigma(i)}$. With these notations we can define ${L}_{match}( hat{y}, y)$ as . ${L}_{match}( hat{y}, y) = ‚àí1_{ {c_{i}= phi }} hat p_{ sigma (i)} (c_{i}) + 1_{ {c_{i}= phi }}{L}_{box} (b_{i}, hat b_{ sigma(i)})$. . Now, to compute the loss function, the Hungarian loss for all pairs matched in the previous step. We can define the loss similarly to the losses of common object detectors, i.e. a linear combination of a negative log-likelihood for class prediction and a box loss. . ${L}_{Hungarian}(y, hat{y}) = sum limits_{i=1}^{N} [ -log hat p_{ sigma (i)} + 1_{ {c_{i}= phi }}{L}_{bbox}(b_{i}, hat b_{ sigma(i)}) ]$. . In actual, the log-probability term when class = $ phi$ is down-weighted by factor of 10 to account for class-imbalance, as there will be very high number of no-objects in the dataset with 100 queries in each image and number of classes being less than that. . Bounding Box Loss . The bounding box loss discussed above is a combination of ${L1}$ loss adn the generalized IOU loss. Unlike tradional object detectors which do box prediction based on initial guess like anchors or proposals, DETR make box predictions directly. . ${L}_{bbox}(b_{i}, hat b_{ sigma(i)}) = lambda_{iou}{L}_{iou}(b_{i}, hat b_{ sigma(i)}) + lambda_{L1}||b_{i}, hat b_{ sigma(i)}|$ where $ lambda_{iou} and lambda_{L1}$ are hyperparameters. . Why Generalized IOU Loss and not just IOU? . The IOU loss is a loss function that measures the similarity between two boxes. The IOU loss is defined as intersection over the union of the two boxes, it has great properties of scale-invariance and have been used as a metric in many tasks of detection and segmentation, but it can‚Äôt be directly used as an objective function to be optimized. Also, there is no strong-corelation between minimizing the commonly used losses (like L2 norm) and the IOU values. . . Let‚Äôs assume a case, where we are calculating loss for bouding box, and consider the top-right point being calculated with L1 loss. As we can see above the IoU is varying in each case, but the distance between the top-right point of predicted and actual box is constant, hence giving same L1 Norm values. . Where as the generalized IoU, takes into account the distance between the predicted box and the actual box even if the boxes are not overlapping. . . The genaralized IoU is defined as $GIoU = IoU - frac{|C-(A cup B)|}{|C|}$ . . Auxiliary Decoding Losses . It was found helpful to use auxiliary losses in decoder during training, especially to help the model output the correct number of objects of each class. We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. We use an additional shared layer-norm to normalize the input to the prediction FFNs from different decoder layers. .",
            "url": "https://abdksyed.github.io/blog/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR.html",
            "relUrl": "/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR.html",
            "date": " ‚Ä¢ Sep 30, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "DETR Q/A",
            "content": ". We take the encoded image (dxH/32xW/32) and send it to Multi-Head Attention (FROM WHERE DO WE TAKE THIS ENCODED IMAGE?) . The encoded image d x H/32 x W/32 is the image which is the output of the transformer encoder. When the final feature map from ResNet-5 block is taken the shape of the feature map is d x H/32 x W/32 and it is converted to embeddings by flattening it on H and W, and transposing it to become, 196x256 (HW x 256) as Transformers accept such sequential embeddings. This embeddings after passed through the 6 layer encoder maintains it‚Äôs shape, if 196x256, and this final encoded 196x256 is again re-arranged to form the Encoded Image which after completion of object detection is sent to a Multi-head attention layer along with bounding box embeddings. . . We than along with dxN Box embeddings send the encoded Image to the Multi-Head Attention . We do something here to generate N x M x H/32 x W/32 maps. (WHAT DO WE DO HERE?) . We perform a Multi Head Attention with the Bounding Box embeddings and the encoded image from the Transformer encoder. . The each box embeddings are dot product with the encoded image, using M attention heads to get the desired N x M x H/32 x W/32 attention heatmap, where N are the number of objects detected in the detection pipeline. . . Then we concatenate these maps with Res5 Block (WHERE IS THIS COMING FROM?) . After getting the attention maps which are small resolution, H/32 and W/32 needs to be up sampled to get the final segmentation image. To achieve the final prediction and increase the resolution, we use a similar concept of Feature Pyramid Networg . Before all, this during the first step of sending the image through the ResNet-50 backbone, we set aside the feature maps after every ResNet block, i.e. after each block of ResNet the output feature maps are saved in separate place, to be used here in the FPN . Res Block 2 -&gt; H/4 x W/4 | Res Block 3 -&gt; H/8 x W/8 | Res Block 4 -&gt; H/16 x W/16 | Res Block 5 -&gt; H/32 x W/32 | . FPN-style Network: . ‚Äã The attention maps are send through a convolutional network, where at first the attention maps are concatenated with the respective size Res Block, i.e., in the beginning the Attention map is of size H/32 x W/32, so the Res5 block which have the feature maps of same size are concatenated. . Hence the Res5/Res4/Res3/Res2 feature maps are coming from the Backbone CNN which was used initially to generate the image embeddings, where each feature map was saved and set aside after every block. . . Then we perform the above steps (EXPLAIN THESE STEPS) . As mentioned above, the attention maps after Multi-Head Attention is concatenated with Res5 block feature maps and is send through a two set of Conv-Norm-Activation layer and than upsampled to become H/16 x W/16. Again the similar process is repeated where corresponding feature maps from ResNet blocks are added and send though Conv-Norm-Activation Layers, and finally a Conv-Norm-Activation-Conv layer is added to get the final attention maps. Here although we use the HxW image, the final attention maps are of the size H/4 x W/4, since in the beginning of ResNet it self we downsample the image by 4 times, hence that‚Äôs the final Mask Logits are 4 times smaller. . The Convolutions are all 3x3 Kernels, and the Normalization being used is Group Normalization and ReLU activation is performed after every Conv-Norm. . . RESULT . This than is passed though Pixel-wise Argmax and concatenated to get the final Panoptic Segmentation Mask. . . Acknowledments: . All Drawings done on draw.io DETR Paper: arxiv Excellent Demo from Author: Video Great Explanation on Paper: Yanic, AI Epiphany .",
            "url": "https://abdksyed.github.io/blog/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR-QA.html",
            "relUrl": "/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR-QA.html",
            "date": " ‚Ä¢ Sep 30, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Creating Custom Dataset using DETR",
            "content": "What and Why? . We at TSAI wanted to train a Panpotic Segmentation Model(DETR) on a custom dataset. We decided to go ahead with Construction Classes and annotated the things of around 50 classes like grader, wheel loader, aac blocks etc. While annotating we just annoatated the things, and left out stuff, as it would be way more tedious to annotate the stuffs like ground, grass, building etc. . We know existing Models are very well trained on COCO dataset, and we could levrage them to predict stuff classes in our images. So we went out and decided to use pre-trained DETR for Panoptic Segmentation to perform inference on our images and get this stuffs for our images. . In total we had 10k Images for all the classes combined, with very high imbalance, like 15 images for one class and on the other extreme 500+ images for other.. . Our Classes for Things and Stuff . Things . We have 48 Things categories. . { &#39;aac_blocks&#39;: 0, &#39;adhesives&#39;: 1, &#39;ahus&#39;: 2, &#39;aluminium_frames_for_false_ceiling&#39;: 3, &#39;chiller&#39;: 4, &#39;concrete_mixer_machine&#39;: 5, &#39;concrete_pump_(50%)&#39;: 6, &#39;control_panel&#39;: 7, &#39;cu_piping&#39;: 8, &#39;distribution_transformer&#39;: 9, &#39;dump_truck___tipper_truck&#39;: 10, &#39;emulsion_paint&#39;: 11, &#39;enamel_paint&#39;: 12, &#39;fine_aggregate&#39;: 13, &#39;fire_buckets&#39;: 14, &#39;fire_extinguishers&#39;: 15, &#39;glass_wool&#39;: 16, &#39;grader&#39;: 17, &#39;hoist&#39;: 18, &#39;hollow_concrete_blocks&#39;: 19, &#39;hot_mix_plant&#39;: 20, &#39;hydra_crane&#39;: 21, &#39;interlocked_switched_socket&#39;: 22, &#39;junction_box&#39;: 23, &#39;lime&#39;: 24, &#39;marble&#39;: 25, &#39;metal_primer&#39;: 26, &#39;pipe_fittings&#39;: 27, &#39;rcc_hume_pipes&#39;: 28, &#39;refrigerant_gas&#39;: 29, &#39;river_sand&#39;: 30, &#39;rmc_batching_plant&#39;: 31, &#39;rmu_units&#39;: 32, &#39;sanitary_fixtures&#39;: 33, &#39;skid_steer_loader_(bobcat)&#39;: 34, &#39;smoke_detectors&#39;: 35, &#39;split_units&#39;: 36, &#39;structural_steel_-_channel&#39;: 37, &#39;switch_boards_and_switches&#39;: 38, &#39;texture_paint&#39;: 39, &#39;threaded_rod&#39;: 40, &#39;transit_mixer&#39;: 41, &#39;vcb_panel&#39;: 42, &#39;vitrified_tiles&#39;: 43, &#39;vrf_units&#39;: 44, &#39;water_tank&#39;: 45, &#39;wheel_loader&#39;: 46, &#39;wood_primer&#39;: 47 } . Stuff . To make life simpler, I decided to make the stuff categories smaller, by collapsing all the categories to their super categories, and finally leavins us with 16 stuff classes . { &#39;building&#39;:48, &#39;ceiling&#39;:49, &#39;floor&#39;:50, &#39;food&#39;:51, &#39;furniture&#39;:52, &#39;ground&#39;:53, &#39;plant&#39;:54, &#39;raw_material&#39;:55, &#39;sky&#39;:56, &#39;solids&#39;:57, &#39;structural&#39;:58, &#39;textile&#39;:59, &#39;wall&#39;:60, &#39;water&#39;:61, &#39;window&#39;:62, &#39;thing&#39;:63 } . Mapping for each stuff category of COCO to their super category: . Categories to Super Categories { &#39;cardboard&#39;: &#39;raw_material&#39;, &#39;paper&#39;: &#39;raw_material&#39;, &#39;plastic&#39;: &#39;raw_material&#39;, &#39;metal&#39;: &#39;raw_material&#39;, &#39;wall-tile&#39;: &#39;wall&#39;, &#39;wall-panel&#39;: &#39;wall&#39;, &#39;wall-wood&#39;: &#39;wall&#39;, &#39;wall-brick&#39;: &#39;wall&#39;, &#39;wall-stone&#39;: &#39;wall&#39;, &#39;wall-concrete&#39;: &#39;wall&#39;, &#39;wall-other&#39;: &#39;wall&#39;, &#39;ceiling-tile&#39;: &#39;ceiling&#39;, &#39;ceiling-other&#39;: &#39;ceiling&#39;, &#39;carpet&#39;: &#39;floor&#39;, &#39;floor-tile&#39;: &#39;floor&#39;, &#39;floor-wood&#39;: &#39;floor&#39;, &#39;floor-marble&#39;: &#39;floor&#39;, &#39;floor-stone&#39;: &#39;floor&#39;, &#39;floor-other&#39;: &#39;floor&#39;, &#39;window-blind&#39;: &#39;window&#39;, &#39;window-other&#39;: &#39;window&#39;, &#39;door-stuff&#39;: &#39;furniture&#39;, &#39;desk-stuff&#39;: &#39;furniture&#39;, &#39;table&#39;: &#39;furniture&#39;, &#39;shelf&#39;: &#39;furniture&#39;, &#39;cabinet&#39;: &#39;furniture&#39;, &#39;cupboard&#39;: &#39;furniture&#39;, &#39;mirror-stuff&#39;: &#39;furniture&#39;, &#39;counter&#39;: &#39;furniture&#39;, &#39;light&#39;: &#39;furniture&#39;, &#39;stairs&#39;: &#39;furniture&#39;, &#39;furniture-other&#39;: &#39;furniture&#39;, &#39;rug&#39;: &#39;textile&#39;, &#39;mat&#39;: &#39;textile&#39;, &#39;towel&#39;: &#39;textile&#39;, &#39;napkin&#39;: &#39;textile&#39;, &#39;clothes&#39;: &#39;textile&#39;, &#39;cloth&#39;: &#39;textile&#39;, &#39;curtain&#39;: &#39;textile&#39;, &#39;blanket&#39;: &#39;textile&#39;, &#39;pillow&#39;: &#39;textile&#39;, &#39;banner&#39;: &#39;textile&#39;,&#39;textile-other&#39;: &#39;textile&#39;, &#39;fruit&#39;: &#39;food&#39;, &#39;salad&#39;: &#39;food&#39;, &#39;vegetable&#39;: &#39;food&#39;, &#39;food-other&#39;: &#39;food&#39;, &#39;house&#39;: &#39;building&#39;, &#39;skyscraper&#39;: &#39;building&#39;,&#39;bridge&#39;: &#39;building&#39;, &#39;tent&#39;: &#39;building&#39;, &#39;roof&#39;: &#39;building&#39;, &#39;building-other&#39;: &#39;building&#39;, &#39;fence&#39;: &#39;structural&#39;, &#39;cage&#39;: &#39;structural&#39;, &#39;net&#39;: &#39;structural&#39;, &#39;railing&#39;: &#39;structural&#39;, &#39;structural-other&#39;: &#39;structural&#39;, &#39;grass&#39;: &#39;plant&#39;, &#39;tree&#39;: &#39;plant&#39;, &#39;bush&#39;: &#39;plant&#39;, &#39;leaves&#39;: &#39;plant&#39;, &#39;flower&#39;: &#39;plant&#39;, &#39;branch&#39;: &#39;plant&#39;, &#39;moss&#39;: &#39;plant&#39;, &#39;straw&#39;: &#39;plant&#39;, &#39;plant-other&#39;: &#39;plant&#39;, &#39;clouds&#39;: &#39;sky&#39;, &#39;sky-other&#39;: &#39;sky&#39;, &#39;wood&#39;: &#39;solids&#39;, &#39;rock&#39;: &#39;solids&#39;, &#39;stone&#39;: &#39;solids&#39;, &#39;mountain&#39;: &#39;solids&#39;, &#39;hill&#39;: &#39;solids&#39;, &#39;solid-other&#39;: &#39;solids&#39;, &#39;sand&#39;: &#39;ground&#39;, &#39;snow&#39;: &#39;ground&#39;, &#39;dirt&#39;: &#39;ground&#39;, &#39;mud&#39;: &#39;ground&#39;, &#39;gravel&#39;: &#39;ground&#39;, &#39;road&#39;: &#39;ground&#39;, &#39;pavement&#39;: &#39;ground&#39;,&#39;railroad&#39;: &#39;ground&#39;, &#39;platform&#39;: &#39;ground&#39;, &#39;playingfield&#39;: &#39;ground&#39;, &#39;ground-other&#39;: &#39;ground&#39;, &#39;fog&#39;: &#39;water&#39;, &#39;river&#39;: &#39;water&#39;, &#39;sea&#39;: &#39;water&#39;, &#39;waterdrops&#39;: &#39;water&#39;, &#39;water-other&#39;: &#39;water&#39;, &#39;things&#39;: &#39;things&#39;, &#39;water&#39;: &#39;water&#39;, &#39;window&#39;: &#39;window&#39;, &#39;ceiling&#39;: &#39;ceiling&#39;, &#39;sky&#39;: &#39;sky&#39;, &#39;floor&#39;: &#39;floor&#39;, &#39;food&#39;: &#39;food&#39;, &#39;building&#39;: &#39;building&#39;,&#39;wall&#39;: &#39;wall&#39; } Annotations . Actual Image . I = Image.open(&lt;image_dir&gt;/&#39;images&#39;/img[&#39;file_name&#39;]) # Sample Image I = I.convert(&#39;RGB&#39;) . . . Our Class Annotation (Segmentation and BBox) . # get all images containing given categories, select one at random catIds = coco.getCatIds(catNms=[&#39;grader&#39;]); # Sample Category imgIds = coco.getImgIds(catIds=catIds ); # Get Image Ids of all images containing the given category img = coco.loadImgs(imgIds[np.random.randint(0,len(imgIds))])[0] #Random Image annIds = coco.getAnnIds(imgIds=img[&#39;id&#39;], catIds=catIds, iscrowd=None) # Get annotation ids of all annotations for the img anns = coco.loadAnns(annIds) coco.showAnns(anns, draw_bbox=True) . . . from matplotlib.patches import Polygon og_poly = [] for ann in anns: # For each annotation poly = np.array(ann[&#39;segmentation&#39;][0]).reshape((int(len(ann[&#39;segmentation&#39;][0])/2), 2)) # Create Array from segmentation poly = Polygon(poly) # Convert to matplotlib Polygon og_poly.append(poly) class_mask = np.zeros((og_w,og_h)) for op in og_poly: cv2.fillPoly(class_mask, pts = np.int32([op.get_xy()]), color =(255)) # Paste our Annotations plt.imshow(class_mask) . . . Model Inference . !git clone -q https://github.com/facebookresearch/detr.git #Facebook DETR import sys import os sys.path.append(os.path.join(os.getcwd(), &quot;detr/&quot;)) # Load DETR trained on COCO for Panoptic Segmentation with ResNet101. model, postprocessor = torch.hub.load(&#39;detr&#39;, &#39;detr_resnet101_panoptic&#39;, source=&#39;local&#39;, pretrained=True, return_postprocessor=True, num_classes=250) model.eval() print(&#39;Loaded!&#39;) img = transform(I).unsqueeze(0) #Resize to 800 Width and Normalize out = model(img) # Model Output . . Attention Maps . # compute the scores, excluding the &quot;no-object&quot; class (the last one) scores = out[&quot;pred_logits&quot;].softmax(-1)[..., :-1].max(-1)[0] # threshold the confidence keep = scores &gt; 0.85 # Plot all the remaining masks ncols = 2 fig, axs = plt.subplots(ncols=ncols, nrows=math.ceil(keep.sum().item() / ncols), figsize=(18, 10)) mask_log_list = [] for i, (attn_map,logit) in enumerate(zip(out[&quot;pred_masks&quot;][keep], out[&quot;pred_logits&quot;][keep])): logit = logit.softmax(-1).argmax().item() if logit &gt; 92: # If stuff of COCO det_id = meta.stuff_dataset_id_to_contiguous_id[logit] logit = meta.stuff_classes[det_id] mask_log_list.append((attn_map,logit)) axs.ravel()[i].imshow(attn_map, cmap=&quot;cividis&quot;) axs.ravel()[i].axis(&#39;off&#39;) fig.tight_layout() . . . As we can see, the model nicely predicts masks for each class. The predictions of the models are car, truck, sand, sky, person and tree. The class maps are pretty darn good. . DETR Post-Processed Mask . # the post-processor expects as input the target size of the predictions (which we set here to the image size) result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0] # We extract the segments info and the panoptic result from DETR&#39;s prediction segments_info = deepcopy(result[&quot;segments_info&quot;]) # Panoptic predictions are stored in a special format png panoptic_seg = Image.open(io.BytesIO(result[&#39;png_string&#39;])) print(panoptic_seg.size) final_w, final_h = panoptic_seg.size # We convert the png into an segment id map panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8) panoptic_seg = torch.from_numpy(rgb2id(panoptic_seg)) # Detectron2 uses a different numbering of coco classes, here we convert the class ids accordingly meta = MetadataCatalog.get(&quot;coco_2017_val_panoptic_separated&quot;) for i in range(len(segments_info)): c = segments_info[i][&quot;category_id&quot;] segments_info[i][&quot;category_id&quot;] = meta.thing_dataset_id_to_contiguous_id[c] if segments_info[i][&quot;isthing&quot;] else meta.stuff_dataset_id_to_contiguous_id[c] # Finally we visualize the prediction v = Visualizer(numpy.array(I.copy().resize((final_w, final_h)))[:, :, ::-1], meta, scale=1.0) v._default_font_size = 20 v = v.draw_panoptic_seg_predictions(panoptic_seg, segments_info, area_threshold=0) cv2_imshow(v.get_image()) . . . . Warning: Woahhhhhhhh, this is Bad! The car, tree, sand, sky and person came out nicely. But the truck is pretty bad as the back area has got leaked into the right region. If we look at the above attention maps now, we see that the region between tree and sand is not identified, the DETR post-processor spreads the masks of nearby class to regions where there are no predictions above the given threshold. . Even after pasting our grader annotation, there will be the truck annotation marked pixels in the image and the sand which is leaked. . . And also the border of our annotations may still have the class as truck, so this masks will cause a problem, when we train our DETR. DETR identifies objects using the edges, as shown in their example. The truck masks may case an issue, where our model may predict our grader as both grader and also truck. . Attention Mask ArgMax Maps . # Taking only Stuff into Consideration # Things of COCO is Void(id=0) for us import itertools import seaborn as sns palette = itertools.cycle(sns.color_palette()) color_list = {} combined_attn = np.zeros(out[&#39;pred_masks&#39;].shape[2:] + (3,)) for attn_map, logit, class_id in mask_log_list: color = (np.asarray(next(palette)) * 255) color_list[class_id] = color combined_attn[attn_map&gt;0] = color combined_attn = combined_attn.astype(np.int) plt.imshow(combined_attn) . . Instead, we can directly ArgMax on the attention maps of the stuff classes, ignoring the things classes of COCO, since we wouldn&#39;t want the same issue as mentioned above, and also avoiding the leakage of class by not using the inbuilt DETR post-processor. . Here the regions which are not predicted, will be marked with black pixels, which in COCO Dataset is void class . . The mask with the BBox(class_id,area). . . Final JSON for All Classes . # Create Annotations of Each Class def run_class(class_images_anns, class_name): class_annotaion = [] class_image = [] class_color = CATEGORY_COLOR[class_name] # Get Color Tuple for the class for img in tqdm(class_images_anns): # For each annotation in the JSON # The Image annotations has .jpg whereas actual Image is png and vice-versa. # try and except to get correct image accordingly try: I = Image.open(dataDir/class_name/&#39;images&#39;/img[&#39;file_name&#39;]) except FileNotFoundError: if img[&#39;file_name&#39;].endswith(&#39;.jpg&#39;): I = Image.open(dataDir/class_name/&#39;images&#39;/img[&#39;file_name&#39;].replace(&#39;jpg&#39;,&#39;png&#39;)) elif img[&#39;file_name&#39;].endswith(&#39;.png&#39;): I = Image.open(dataDir/class_name/&#39;images&#39;/img[&#39;file_name&#39;].replace(&#39;png&#39;,&#39;jpg&#39;)) # Convert any grayscale or RBGA to RGB I = I.convert(&#39;RGB&#39;) og_h, og_w = I.size # Get Annotation of the Image annIds = coco.getAnnIds(imgIds=img[&#39;id&#39;], catIds=catIds, iscrowd=None) anns = coco.loadAnns(annIds) # Create Polygon for custom Annotation. og_poly = gen_original_poly(anns) # Get DETR output on our Image w.r.t COCO classes trans_img = transform(I).unsqueeze(0) out = model(trans_img.to(&#39;cuda&#39;)) # Create Masks by stacking Attention Maps and Pasting our Annotation # Excluding the functions definition for brevity. Can be found from colab link. class_masks = generate_class_maps(out) pred_mask, color2class = generate_pred_masks(class_masks, out[&#39;pred_masks&#39;].shape[2:]) pred_mask = cv2.resize(pred_mask, (og_h, og_w), interpolation= cv2.INTER_NEAREST) #Pasting Our Class on Mask for op in og_poly: cv2.fillPoly(pred_mask, pts = np.int32([op.get_xy()]), color = class_color) #Convering Mask to ID using panopticapi.utils mask_id = rgb2id(pred_mask) # Final Segmentation Details segments_info = generate_gt(mask_id, color2class, class_name) # The ID image(1 Channel) converted to 3 Channel Mask to save. img_save = Image.fromarray(id2rgb(mask_id)) mask_file_name = img[&#39;file_name&#39;].split(&#39;.&#39;)[0] + &#39;.png&#39; img_save.save(dataDir/class_name/&#39;annotations&#39;/mask_file_name) # Appending the Image Annotation to List class_annotaion.append( { &quot;segments_info&quot;: segments_info, &quot;file_name&quot;: mask_file_name, &quot;image_id&quot;: int(img[&#39;id&#39;]) } ) return class_annotaion, class_image . . for class_name in list_class: # Loop over all the classes names annFile = &lt;images_dir&gt;/class_name/&#39;coco.json&#39; # Path to the annotations file of each class coco = COCO(annFile) # Convert JSON to coco object (pycocotools.COCO) cats = coco.loadCats(coco.getCatIds()) # get all images containing given categories, select one at random catIds = coco.getCatIds(catNms=[class_name]); imgIds = coco.getImgIds(catIds=catIds); images = coco.loadImgs(imgIds) try: os.mkdir(&lt;images_dir&gt;/class_name/&#39;annotations&#39;) # Create Annotations Folder for each Class except FileExistsError as e: print(&#39;WARNING!&#39;, e) CLASS_ANNOTATION = run_class(images, class_name) # Generate Annotations for each class FINAL_JSON = {} FINAL_JSON[&#39;licenses&#39;] = coco.dataset[&#39;licenses&#39;] FINAL_JSON[&#39;info&#39;] = coco.dataset[&#39;info&#39;] FINAL_JSON[&#39;categories&#39;] = CATEGORIES FINAL_JSON[&#39;images&#39;] = coco.dataset[&#39;images&#39;] FINAL_JSON[&#39;annotations&#39;] = CLASS_ANNOTATION out_file = open(&lt;images_dir&gt;/class_name/&#39;annotations&#39;/f&#39;{class_name}.json&#39;, &quot;w&quot;) json.dump(FINAL_JSON, out_file, indent = 4) out_file.close() . . COCO Format . COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features with 80 object(things) classes, and 91 stuff classes for several tasks like captioning, segmentation, detection etc. . And it is the most widely used data as well as most widely used data format. So converting our annotations to the COCO format, would help us in levraging pre-built tools to create data pipelines to model. . File Structure . &lt;dataset_dir&gt;/ data/ &lt;filename0&gt;.&lt;ext&gt; &lt;filename1&gt;.&lt;ext&gt; ... annotations/ &lt;filename0&gt;.png &lt;filename1&gt;.png ... labels.json . The data folder has all the images, the images can be in image formats like JPG, JPEG, PNG. The annotations folder has the images of the masks, for every image in the data folder, with the same name and .png extension. The stem name of the file should match with the image in the data. The labels.json has 5 main keys, info, licenses, categories, images and annotations. This json file holds the data for the ground truth of the images. The format of the JSON can be varying as per the problem like object-detection, segmentation, keypoint-detection or image-captioning. . { &quot;info&quot;: info, &quot;licenses&quot;: [license], &quot;categories&quot;: [categories] &quot;images&quot;: [image], &quot;annotations&quot;: [annotation], } . The images, info and licenses remains same for all types, where as the annotations and categories format will differ. . info = { &quot;year&quot;: int, &quot;version&quot;: str, &quot;description&quot;: str, &quot;contributor&quot;: str, &quot;url&quot;: str, &quot;date_created&quot;: datetime, } image = { &quot;id&quot;: int, &quot;width&quot;: int, &quot;height&quot;: int, &quot;file_name&quot;: str, &quot;license&quot;: int, &quot;flickr_url&quot;: str, &quot;coco_url&quot;: str, &quot;date_captured&quot;: datetime, } license = { &quot;id&quot;: int, &quot;name&quot;: str, &quot;url&quot;: str, } . . Object Detection . Each object instance annotation contains a series of fields, including the category id and segmentation mask(optional if only Detection) of the object. An enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed). Finally, the categories field of the annotation structure stores the mapping of category id to category and supercategory names . annotation = { &quot;id&quot;: int, &quot;image_id&quot;: int, &quot;category_id&quot;: int, &quot;segmentation&quot;: RLE or [polygon], &quot;area&quot;: float, &quot;bbox&quot;: [x,y,width,height], &quot;iscrowd&quot;: 0 or 1, } categories = [{ &quot;id&quot;: int, &quot;name&quot;: str, &quot;supercategory&quot;: str, }] . . Panoptic Segmentation . For the panoptic task, each annotation struct is a per-image annotation rather than a per-object annotation. Each per-image annotation has two parts: (1) a PNG that stores the class-agnostic image segmentation and (2) a JSON struct that stores the semantic information for each image segment . To match an annotation with an image, use the image_id field (that is annotation.image_id==image.id). | For each annotation, per-pixel segment ids are stored as a single PNG as the same name as image. | Each segment (whether it&#39;s a stuff or thing segment) is assigned a unique id. | Unlabeled pixels (void) are assigned a value of 0. Note that when you load the PNG as an RGB image, you will need to compute the ids via ids=R+G*256+B*256^2. | In annotation file. The segment_info.id stores the unique id of the segment and is used to retrieve the corresponding mask from the PNG (ids==segment_info.id). | Finally, each category struct has two additional fields: isthing that distinguishes stuff and thing categories and color that is useful for consistent visualization. | . annotation = { &quot;image_id&quot;: int, &quot;file_name&quot;: str, &quot;segments_info&quot;: [segment_info], } segment_info = { &quot;id&quot;: int, &quot;category_id&quot;: int, &quot;area&quot;: int, &quot;bbox&quot;: [x,y,width,height], &quot;iscrowd&quot;: 0 or 1, } categories = [{ &quot;id&quot;: int, &quot;name&quot;: str, &quot;supercategory&quot;: str, &quot;isthing&quot;: 0 or 1, &quot;color&quot;: [R,G,B], }] . .",
            "url": "https://abdksyed.github.io/blog/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/09/30/CustomDataset.html",
            "relUrl": "/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/09/30/CustomDataset.html",
            "date": " ‚Ä¢ Sep 30, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://abdksyed.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://abdksyed.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://abdksyed.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://abdksyed.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}