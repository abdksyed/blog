{
  
    
        "post0": {
            "title": "DE⫶TR -- Extending Object Detection to Panoptic Segmentation",
            "content": ".",
            "url": "https://abdksyed.github.io/blog/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/03/PanopticSegmentation-DETR.html",
            "relUrl": "/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/03/PanopticSegmentation-DETR.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "DE⫶TR -- End-to-End Object Detection with Transformers",
            "content": ".",
            "url": "https://abdksyed.github.io/blog/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/02/ObjectDetection-DETR.html",
            "relUrl": "/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/10/02/ObjectDetection-DETR.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "DETR",
            "content": "What is DETR? . Finds let’s know what is Object Detection . Object detection refers to the capability of computer and software systems to locate objects in an image/scene and identify each object. Object detection has been widely used for face detection, vehicle detection, pedestrian counting, web images, security systems and driverless cars. . How DETR approach the problem? . DETR streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. . Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. . DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner . Another helpful aspect unlike most existing detection methods, DETR doesn’t require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes. . Architecture . Transformer - Attention is all you need. . Transformers introduced in 2017, came up with a novel approach of Self-Attention Mechanism where each element scan through every other element of a sequence and update it by aggregating information from the whole sequence. . The Transformers changed the entire NLP domain, but wasn’t used until 2020 in the Computer Vision domain by the introduction of Vision Transformers. (more on it here). . . DETR uses the same Self-Attention Mechanism in their Encoded-Decoder architecture. . In ViT the images are converted to patches and than flattened and passed through the Linear Projection layer. to get image embeddings which are added with positional embeddings. But in DETR, we use a ResNet50 trained on ImageNet as a Backbone, and the final feature map from ResNet-5 block is taken the shape of the feature map is d x H/32 x W/32 and it is converted to embeddings by flattening it on H and W, and transposing it to become, 196x256 (HW x 256) as Transformers accept such sequential embeddings. . DETR uses Encoder-Decoder architecture, Each encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network (FFN). Since the transformer architecture is permutation-invariant, it is supplemented with fixed positional encodings that are added to the input of each attention layer. DETR used 6 encoders. . TThe decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed encoder-decoder attention and self-attention mechanisms, with the only difference that DETR model decodes the N objects in parallel at each decoder layer, while original Transformer use an autoregressive model that predicts the output sequence one element at a time. . Since the decoder is also permutation-invariant, the N input embeddings must be different to produce different results. These input embeddings(to the decoder) are learnt positional encodings that are refer to as object queries, and similarly to the encoder, we add them to the input of each attention layer. The N object queries are transformed into an output embedding by the decoder. They are then independently decoded into box coordinates and class labels by a feed forward network, resulting N final predictions. Using self and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context. . . Prediction through Feed Forward Network . The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer. The FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image, and the linear layer predicts the class label using a softmax function. Since we predict a fixed-size set of N bounding boxes, where N is usually much larger than the actual number of objects of interest in an image, an additional special class label ∅ is used to represent that no object is detected within a slot. This class plays a similar role to the “background” class in the standard object detection approaches. . LOSS . Let’s see how DETR is trained . Set Prediction Loss . DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image. Since there are large number of predictions, which as said is more than objects present in the image, one of the main difficulties arises in training is to score predicted objects (class, position, size) with respect to the ground truth. DETR loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses, more specifically the Hungarian algorithm which is a “combinatorial optimization algorithm that solves the assignment problem in polynomial time”. ref . One of the main benefits of this approach is that it simply produces a set of predictions rather than a list, meaning that the produced boxes are unique (which saves a lot of post-processing time). It also allows them to do box predictions directly rather than doing those predictions with respect to some initial guesses. . The matching is to account for ordering differences in the permutations of the predictions compared to the ground truth. Given a particular loss function ${L}_{match}( hat{y}, y)$, it finds the permutation for the predictions that gives the minimum total loss. The matching checks the possibilities of all permutations, and selects the one that minimizes the total loss, giving the model the benifit of best possible matching to set of predictions. . . This matching portion plays the same role as heuristic rules used to match proposal or anchors to past ground truth objects in past object detection models. The solution for the above problem is found using the Hungarain Algorithm. As can be seen from the above image, the name for the loss function comes from the Bipartite Graph that is seen in graph theory. . σ^=argmin⁡σϵN∑i=1NLmatch(y^i,yi) hat{ sigma} = arg min limits_{ sigma epsilon N} sum limits_{i=1}^{N} L_{match}( hat{y}_i, y_i)σ^=argσϵNmin​i=1∑N​Lmatch​(y^​i​,yi​) . Matching Loss . We have seen that the bipartite matching, tries to calculate the matching loss and find the set of matching which gives the least sum of matching loss of each element. Let’s see what is the Matching loss being used here. . ${L}_{match}( hat{y}, y)$ is a pair-wise matching cost between ground truth $y_{i}$ and a prediction with index $ sigma (i)$. The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. Each element i of the ground truth set can be seen as a $y_{i} = (c_{i}, b_{i})$ where $c_{i}$ is the target class label (which may be ∅) and $b_{i}$ ∈ [0, 1] is a vector that defines ground truth box center coordinates and its height and width relative to the image size. For the prediction with index $ sigma (i)$ we define probability of class $c_{i}$ as $ hat p_{ sigma(i)}(c_{i})$ and the predicted box as $ hat b_{ sigma(i)}$. With these notations we can define ${L}_{match}( hat{y}, y)$ as . Lmatch(y^,y)=−1{ci=ϕ}p^σ(i)(ci)+1{ci=ϕ}Lbox(bi,b^σ(i)){L}_{match}( hat{y}, y) = −1_{ {c_{i}= phi }} hat p_{ sigma (i)} (c_{i}) + 1_{ {c_{i}= phi }}{L}_{box} (b_{i}, hat b_{ sigma(i)})Lmatch​(y^​,y)=−1{ci​=ϕ}​p^​σ(i)​(ci​)+1{ci​=ϕ}​Lbox​(bi​,b^σ(i)​). . Now, to compute the loss function, the Hungarian loss for all pairs matched in the previous step. We can define the loss similarly to the losses of common object detectors, i.e. a linear combination of a negative log-likelihood for class prediction and a box loss. . LHungarian(y,y^)=∑i=1N[−logp^σ(i)+1{ci=ϕ}Lbbox(bi,b^σ(i))]{L}_{Hungarian}(y, hat{y}) = sum limits_{i=1}^{N} [ -log hat p_{ sigma (i)} + 1_{ {c_{i}= phi }}{L}_{bbox}(b_{i}, hat b_{ sigma(i)}) ]LHungarian​(y,y^​)=i=1∑N​[−logp^​σ(i)​+1{ci​=ϕ}​Lbbox​(bi​,b^σ(i)​)]. . In actual, the log-probability term when class = $ phi$ is down-weighted by factor of 10 to account for class-imbalance, as there will be very high number of no-objects in the dataset with 100 queries in each image and number of classes being less than that. . Bounding Box Loss . The bounding box loss discussed above is a combination of ${L1}$ loss adn the generalized IOU loss. Unlike tradional object detectors which do box prediction based on initial guess like anchors or proposals, DETR make box predictions directly. . Lbbox(bi,b^σ(i))=λiouLiou(bi,b^σ(i))+λL1∣∣bi,b^σ(i)∣{L}_{bbox}(b_{i}, hat b_{ sigma(i)}) = lambda_{iou}{L}_{iou}(b_{i}, hat b_{ sigma(i)}) + lambda_{L1}||b_{i}, hat b_{ sigma(i)}|Lbbox​(bi​,b^σ(i)​)=λiou​Liou​(bi​,b^σ(i)​)+λL1​∣∣bi​,b^σ(i)​∣ where $ lambda_{iou} and lambda_{L1}$ are hyperparameters. . Why Generalized IOU Loss and not just IOU? . The IOU loss is a loss function that measures the similarity between two boxes. The IOU loss is defined as intersection over the union of the two boxes, it has great properties of scale-invariance and have been used as a metric in many tasks of detection and segmentation, but it can’t be directly used as an objective function to be optimized. Also, there is no strong-corelation between minimizing the commonly used losses (like L2 norm) and the IOU values. . . Let’s assume a case, where we are calculating loss for bouding box, and consider the top-right point being calculated with L1 loss. As we can see above the IoU is varying in each case, but the distance between the top-right point of predicted and actual box is constant, hence giving same L1 Norm values. . Where as the generalized IoU, takes into account the distance between the predicted box and the actual box even if the boxes are not overlapping. . . The genaralized IoU is defined as GIoU=IoU−∣C−(A∪B)∣∣C∣GIoU = IoU - frac{|C-(A cup B)|}{|C|}GIoU=IoU−∣C∣∣C−(A∪B)∣​ . . Auxiliary Decoding Losses . It was found helpful to use auxiliary losses in decoder during training, especially to help the model output the correct number of objects of each class. We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. We use an additional shared layer-norm to normalize the input to the prediction FFNs from different decoder layers. .",
            "url": "https://abdksyed.github.io/blog/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR.html",
            "relUrl": "/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "DETR Q/A",
            "content": ". We take the encoded image (dxH/32xW/32) and send it to Multi-Head Attention (FROM WHERE DO WE TAKE THIS ENCODED IMAGE?) . The encoded image d x H/32 x W/32 is the image which is the output of the transformer encoder. When the final feature map from ResNet-5 block is taken the shape of the feature map is d x H/32 x W/32 and it is converted to embeddings by flattening it on H and W, and transposing it to become, 196x256 (HW x 256) as Transformers accept such sequential embeddings. This embeddings after passed through the 6 layer encoder maintains it’s shape, if 196x256, and this final encoded 196x256 is again re-arranged to form the Encoded Image which after completion of object detection is sent to a Multi-head attention layer along with bounding box embeddings. . . We than along with dxN Box embeddings send the encoded Image to the Multi-Head Attention . We do something here to generate N x M x H/32 x W/32 maps. (WHAT DO WE DO HERE?) . We perform a Multi Head Attention with the Bounding Box embeddings and the encoded image from the Transformer encoder. . The each box embeddings are dot product with the encoded image, using M attention heads to get the desired N x M x H/32 x W/32 attention heatmap, where N are the number of objects detected in the detection pipeline. . . Then we concatenate these maps with Res5 Block (WHERE IS THIS COMING FROM?) . After getting the attention maps which are small resolution, H/32 and W/32 needs to be up sampled to get the final segmentation image. To achieve the final prediction and increase the resolution, we use a similar concept of Feature Pyramid Networg . Before all, this during the first step of sending the image through the ResNet-50 backbone, we set aside the feature maps after every ResNet block, i.e. after each block of ResNet the output feature maps are saved in separate place, to be used here in the FPN . Res Block 2 -&gt; H/4 x W/4 | Res Block 3 -&gt; H/8 x W/8 | Res Block 4 -&gt; H/16 x W/16 | Res Block 5 -&gt; H/32 x W/32 | . FPN-style Network: . ​ The attention maps are send through a convolutional network, where at first the attention maps are concatenated with the respective size Res Block, i.e., in the beginning the Attention map is of size H/32 x W/32, so the Res5 block which have the feature maps of same size are concatenated. . Hence the Res5/Res4/Res3/Res2 feature maps are coming from the Backbone CNN which was used initially to generate the image embeddings, where each feature map was saved and set aside after every block. . . Then we perform the above steps (EXPLAIN THESE STEPS) . As mentioned above, the attention maps after Multi-Head Attention is concatenated with Res5 block feature maps and is send through a two set of Conv-Norm-Activation layer and than upsampled to become H/16 x W/16. Again the similar process is repeated where corresponding feature maps from ResNet blocks are added and send though Conv-Norm-Activation Layers, and finally a Conv-Norm-Activation-Conv layer is added to get the final attention maps. Here although we use the HxW image, the final attention maps are of the size H/4 x W/4, since in the beginning of ResNet it self we downsample the image by 4 times, hence that’s the final Mask Logits are 4 times smaller. . The Convolutions are all 3x3 Kernels, and the Normalization being used is Group Normalization and ReLU activation is performed after every Conv-Norm. . . RESULT . This than is passed though Pixel-wise Argmax and concatenated to get the final Panoptic Segmentation Mask. . . Acknowledments: . All Drawings done on draw.io DETR Paper: arxiv Excellent Demo from Author: Video Great Explanation on Paper: Yanic, AI Epiphany .",
            "url": "https://abdksyed.github.io/blog/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR-QA.html",
            "relUrl": "/detr/panoptic%20segmentation/object%20detection/2021/09/30/DETR-QA.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Creating Custom Dataset using DETR",
            "content": "What and Why? . We at TSAI wanted to train a Panpotic Segmentation Model(DETR) on a custom dataset. We decided to go ahead with Construction Classes and annotated the things of around 50 classes like grader, wheel loader, aac blocks etc. While annotating we just annoatated the things, and left out stuff, as it would be way more tedious to annotate the stuffs like ground, grass, building etc. . We know existing Models are very well trained on COCO dataset, and we could levrage them to predict stuff classes in our images. So we went out and decided to use pre-trained DETR for Panoptic Segmentation to perform inference on our images and get this stuffs for our images. . In total we had 10k Images for all the classes combined, with very high imbalance, like 15 images for one class and on the other extreme 500+ images for other.. . Our Classes for Things and Stuff . Things . We have 48 Things categories. . { &#39;aac_blocks&#39;: 0, &#39;adhesives&#39;: 1, &#39;ahus&#39;: 2, &#39;aluminium_frames_for_false_ceiling&#39;: 3, &#39;chiller&#39;: 4, &#39;concrete_mixer_machine&#39;: 5, &#39;concrete_pump_(50%)&#39;: 6, &#39;control_panel&#39;: 7, &#39;cu_piping&#39;: 8, &#39;distribution_transformer&#39;: 9, &#39;dump_truck___tipper_truck&#39;: 10, &#39;emulsion_paint&#39;: 11, &#39;enamel_paint&#39;: 12, &#39;fine_aggregate&#39;: 13, &#39;fire_buckets&#39;: 14, &#39;fire_extinguishers&#39;: 15, &#39;glass_wool&#39;: 16, &#39;grader&#39;: 17, &#39;hoist&#39;: 18, &#39;hollow_concrete_blocks&#39;: 19, &#39;hot_mix_plant&#39;: 20, &#39;hydra_crane&#39;: 21, &#39;interlocked_switched_socket&#39;: 22, &#39;junction_box&#39;: 23, &#39;lime&#39;: 24, &#39;marble&#39;: 25, &#39;metal_primer&#39;: 26, &#39;pipe_fittings&#39;: 27, &#39;rcc_hume_pipes&#39;: 28, &#39;refrigerant_gas&#39;: 29, &#39;river_sand&#39;: 30, &#39;rmc_batching_plant&#39;: 31, &#39;rmu_units&#39;: 32, &#39;sanitary_fixtures&#39;: 33, &#39;skid_steer_loader_(bobcat)&#39;: 34, &#39;smoke_detectors&#39;: 35, &#39;split_units&#39;: 36, &#39;structural_steel_-_channel&#39;: 37, &#39;switch_boards_and_switches&#39;: 38, &#39;texture_paint&#39;: 39, &#39;threaded_rod&#39;: 40, &#39;transit_mixer&#39;: 41, &#39;vcb_panel&#39;: 42, &#39;vitrified_tiles&#39;: 43, &#39;vrf_units&#39;: 44, &#39;water_tank&#39;: 45, &#39;wheel_loader&#39;: 46, &#39;wood_primer&#39;: 47 } . Stuff . To make life simpler, I decided to make the stuff categories smaller, by collapsing all the categories to their super categories, and finally leavins us with 16 stuff classes . { &#39;building&#39;:48, &#39;ceiling&#39;:49, &#39;floor&#39;:50, &#39;food&#39;:51, &#39;furniture&#39;:52, &#39;ground&#39;:53, &#39;plant&#39;:54, &#39;raw_material&#39;:55, &#39;sky&#39;:56, &#39;solids&#39;:57, &#39;structural&#39;:58, &#39;textile&#39;:59, &#39;wall&#39;:60, &#39;water&#39;:61, &#39;window&#39;:62, &#39;thing&#39;:63 } . Mapping for each stuff category of COCO to their super category: . Categories to Super Categories { &#39;cardboard&#39;: &#39;raw_material&#39;, &#39;paper&#39;: &#39;raw_material&#39;, &#39;plastic&#39;: &#39;raw_material&#39;, &#39;metal&#39;: &#39;raw_material&#39;, &#39;wall-tile&#39;: &#39;wall&#39;, &#39;wall-panel&#39;: &#39;wall&#39;, &#39;wall-wood&#39;: &#39;wall&#39;, &#39;wall-brick&#39;: &#39;wall&#39;, &#39;wall-stone&#39;: &#39;wall&#39;, &#39;wall-concrete&#39;: &#39;wall&#39;, &#39;wall-other&#39;: &#39;wall&#39;, &#39;ceiling-tile&#39;: &#39;ceiling&#39;, &#39;ceiling-other&#39;: &#39;ceiling&#39;, &#39;carpet&#39;: &#39;floor&#39;, &#39;floor-tile&#39;: &#39;floor&#39;, &#39;floor-wood&#39;: &#39;floor&#39;, &#39;floor-marble&#39;: &#39;floor&#39;, &#39;floor-stone&#39;: &#39;floor&#39;, &#39;floor-other&#39;: &#39;floor&#39;, &#39;window-blind&#39;: &#39;window&#39;, &#39;window-other&#39;: &#39;window&#39;, &#39;door-stuff&#39;: &#39;furniture&#39;, &#39;desk-stuff&#39;: &#39;furniture&#39;, &#39;table&#39;: &#39;furniture&#39;, &#39;shelf&#39;: &#39;furniture&#39;, &#39;cabinet&#39;: &#39;furniture&#39;, &#39;cupboard&#39;: &#39;furniture&#39;, &#39;mirror-stuff&#39;: &#39;furniture&#39;, &#39;counter&#39;: &#39;furniture&#39;, &#39;light&#39;: &#39;furniture&#39;, &#39;stairs&#39;: &#39;furniture&#39;, &#39;furniture-other&#39;: &#39;furniture&#39;, &#39;rug&#39;: &#39;textile&#39;, &#39;mat&#39;: &#39;textile&#39;, &#39;towel&#39;: &#39;textile&#39;, &#39;napkin&#39;: &#39;textile&#39;, &#39;clothes&#39;: &#39;textile&#39;, &#39;cloth&#39;: &#39;textile&#39;, &#39;curtain&#39;: &#39;textile&#39;, &#39;blanket&#39;: &#39;textile&#39;, &#39;pillow&#39;: &#39;textile&#39;, &#39;banner&#39;: &#39;textile&#39;,&#39;textile-other&#39;: &#39;textile&#39;, &#39;fruit&#39;: &#39;food&#39;, &#39;salad&#39;: &#39;food&#39;, &#39;vegetable&#39;: &#39;food&#39;, &#39;food-other&#39;: &#39;food&#39;, &#39;house&#39;: &#39;building&#39;, &#39;skyscraper&#39;: &#39;building&#39;,&#39;bridge&#39;: &#39;building&#39;, &#39;tent&#39;: &#39;building&#39;, &#39;roof&#39;: &#39;building&#39;, &#39;building-other&#39;: &#39;building&#39;, &#39;fence&#39;: &#39;structural&#39;, &#39;cage&#39;: &#39;structural&#39;, &#39;net&#39;: &#39;structural&#39;, &#39;railing&#39;: &#39;structural&#39;, &#39;structural-other&#39;: &#39;structural&#39;, &#39;grass&#39;: &#39;plant&#39;, &#39;tree&#39;: &#39;plant&#39;, &#39;bush&#39;: &#39;plant&#39;, &#39;leaves&#39;: &#39;plant&#39;, &#39;flower&#39;: &#39;plant&#39;, &#39;branch&#39;: &#39;plant&#39;, &#39;moss&#39;: &#39;plant&#39;, &#39;straw&#39;: &#39;plant&#39;, &#39;plant-other&#39;: &#39;plant&#39;, &#39;clouds&#39;: &#39;sky&#39;, &#39;sky-other&#39;: &#39;sky&#39;, &#39;wood&#39;: &#39;solids&#39;, &#39;rock&#39;: &#39;solids&#39;, &#39;stone&#39;: &#39;solids&#39;, &#39;mountain&#39;: &#39;solids&#39;, &#39;hill&#39;: &#39;solids&#39;, &#39;solid-other&#39;: &#39;solids&#39;, &#39;sand&#39;: &#39;ground&#39;, &#39;snow&#39;: &#39;ground&#39;, &#39;dirt&#39;: &#39;ground&#39;, &#39;mud&#39;: &#39;ground&#39;, &#39;gravel&#39;: &#39;ground&#39;, &#39;road&#39;: &#39;ground&#39;, &#39;pavement&#39;: &#39;ground&#39;,&#39;railroad&#39;: &#39;ground&#39;, &#39;platform&#39;: &#39;ground&#39;, &#39;playingfield&#39;: &#39;ground&#39;, &#39;ground-other&#39;: &#39;ground&#39;, &#39;fog&#39;: &#39;water&#39;, &#39;river&#39;: &#39;water&#39;, &#39;sea&#39;: &#39;water&#39;, &#39;waterdrops&#39;: &#39;water&#39;, &#39;water-other&#39;: &#39;water&#39;, &#39;things&#39;: &#39;things&#39;, &#39;water&#39;: &#39;water&#39;, &#39;window&#39;: &#39;window&#39;, &#39;ceiling&#39;: &#39;ceiling&#39;, &#39;sky&#39;: &#39;sky&#39;, &#39;floor&#39;: &#39;floor&#39;, &#39;food&#39;: &#39;food&#39;, &#39;building&#39;: &#39;building&#39;,&#39;wall&#39;: &#39;wall&#39; } Annotations . Actual Image . I = Image.open(&lt;image_dir&gt;/&#39;images&#39;/img[&#39;file_name&#39;]) # Sample Image I = I.convert(&#39;RGB&#39;) . . . Our Class Annotation (Segmentation and BBox) . # get all images containing given categories, select one at random catIds = coco.getCatIds(catNms=[&#39;grader&#39;]); # Sample Category imgIds = coco.getImgIds(catIds=catIds ); # Get Image Ids of all images containing the given category img = coco.loadImgs(imgIds[np.random.randint(0,len(imgIds))])[0] #Random Image annIds = coco.getAnnIds(imgIds=img[&#39;id&#39;], catIds=catIds, iscrowd=None) # Get annotation ids of all annotations for the img anns = coco.loadAnns(annIds) coco.showAnns(anns, draw_bbox=True) . . . from matplotlib.patches import Polygon og_poly = [] for ann in anns: # For each annotation poly = np.array(ann[&#39;segmentation&#39;][0]).reshape((int(len(ann[&#39;segmentation&#39;][0])/2), 2)) # Create Array from segmentation poly = Polygon(poly) # Convert to matplotlib Polygon og_poly.append(poly) class_mask = np.zeros((og_w,og_h)) for op in og_poly: cv2.fillPoly(class_mask, pts = np.int32([op.get_xy()]), color =(255)) # Paste our Annotations plt.imshow(class_mask) . . . Model Inference . !git clone -q https://github.com/facebookresearch/detr.git #Facebook DETR import sys import os sys.path.append(os.path.join(os.getcwd(), &quot;detr/&quot;)) # Load DETR trained on COCO for Panoptic Segmentation with ResNet101. model, postprocessor = torch.hub.load(&#39;detr&#39;, &#39;detr_resnet101_panoptic&#39;, source=&#39;local&#39;, pretrained=True, return_postprocessor=True, num_classes=250) model.eval() print(&#39;Loaded!&#39;) img = transform(I).unsqueeze(0) #Resize to 800 Width and Normalize out = model(img) # Model Output . . Attention Maps . # compute the scores, excluding the &quot;no-object&quot; class (the last one) scores = out[&quot;pred_logits&quot;].softmax(-1)[..., :-1].max(-1)[0] # threshold the confidence keep = scores &gt; 0.85 # Plot all the remaining masks ncols = 2 fig, axs = plt.subplots(ncols=ncols, nrows=math.ceil(keep.sum().item() / ncols), figsize=(18, 10)) mask_log_list = [] for i, (attn_map,logit) in enumerate(zip(out[&quot;pred_masks&quot;][keep], out[&quot;pred_logits&quot;][keep])): logit = logit.softmax(-1).argmax().item() if logit &gt; 92: # If stuff of COCO det_id = meta.stuff_dataset_id_to_contiguous_id[logit] logit = meta.stuff_classes[det_id] mask_log_list.append((attn_map,logit)) axs.ravel()[i].imshow(attn_map, cmap=&quot;cividis&quot;) axs.ravel()[i].axis(&#39;off&#39;) fig.tight_layout() . . . As we can see, the model nicely predicts masks for each class. The predictions of the models are car, truck, sand, sky, person and tree. The class maps are pretty darn good. . DETR Post-Processed Mask . # the post-processor expects as input the target size of the predictions (which we set here to the image size) result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0] # We extract the segments info and the panoptic result from DETR&#39;s prediction segments_info = deepcopy(result[&quot;segments_info&quot;]) # Panoptic predictions are stored in a special format png panoptic_seg = Image.open(io.BytesIO(result[&#39;png_string&#39;])) print(panoptic_seg.size) final_w, final_h = panoptic_seg.size # We convert the png into an segment id map panoptic_seg = numpy.array(panoptic_seg, dtype=numpy.uint8) panoptic_seg = torch.from_numpy(rgb2id(panoptic_seg)) # Detectron2 uses a different numbering of coco classes, here we convert the class ids accordingly meta = MetadataCatalog.get(&quot;coco_2017_val_panoptic_separated&quot;) for i in range(len(segments_info)): c = segments_info[i][&quot;category_id&quot;] segments_info[i][&quot;category_id&quot;] = meta.thing_dataset_id_to_contiguous_id[c] if segments_info[i][&quot;isthing&quot;] else meta.stuff_dataset_id_to_contiguous_id[c] # Finally we visualize the prediction v = Visualizer(numpy.array(I.copy().resize((final_w, final_h)))[:, :, ::-1], meta, scale=1.0) v._default_font_size = 20 v = v.draw_panoptic_seg_predictions(panoptic_seg, segments_info, area_threshold=0) cv2_imshow(v.get_image()) . . . . Warning: Woahhhhhhhh, this is Bad! The car, tree, sand, sky and person came out nicely. But the truck is pretty bad as the back area has got leaked into the right region. If we look at the above attention maps now, we see that the region between tree and sand is not identified, the DETR post-processor spreads the masks of nearby class to regions where there are no predictions above the given threshold. . Even after pasting our grader annotation, there will be the truck annotation marked pixels in the image and the sand which is leaked. . . And also the border of our annotations may still have the class as truck, so this masks will cause a problem, when we train our DETR. DETR identifies objects using the edges, as shown in their example. The truck masks may case an issue, where our model may predict our grader as both grader and also truck. . Attention Mask ArgMax Maps . # Taking only Stuff into Consideration # Things of COCO is Void(id=0) for us import itertools import seaborn as sns palette = itertools.cycle(sns.color_palette()) color_list = {} combined_attn = np.zeros(out[&#39;pred_masks&#39;].shape[2:] + (3,)) for attn_map, logit, class_id in mask_log_list: color = (np.asarray(next(palette)) * 255) color_list[class_id] = color combined_attn[attn_map&gt;0] = color combined_attn = combined_attn.astype(np.int) plt.imshow(combined_attn) . . Instead, we can directly ArgMax on the attention maps of the stuff classes, ignoring the things classes of COCO, since we wouldn&#39;t want the same issue as mentioned above, and also avoiding the leakage of class by not using the inbuilt DETR post-processor. . Here the regions which are not predicted, will be marked with black pixels, which in COCO Dataset is void class . . The mask with the BBox(class_id,area). . . Final JSON for All Classes . # Create Annotations of Each Class def run_class(class_images_anns, class_name): class_annotaion = [] class_image = [] class_color = CATEGORY_COLOR[class_name] # Get Color Tuple for the class for img in tqdm(class_images_anns): # For each annotation in the JSON # The Image annotations has .jpg whereas actual Image is png and vice-versa. # try and except to get correct image accordingly try: I = Image.open(dataDir/class_name/&#39;images&#39;/img[&#39;file_name&#39;]) except FileNotFoundError: if img[&#39;file_name&#39;].endswith(&#39;.jpg&#39;): I = Image.open(dataDir/class_name/&#39;images&#39;/img[&#39;file_name&#39;].replace(&#39;jpg&#39;,&#39;png&#39;)) elif img[&#39;file_name&#39;].endswith(&#39;.png&#39;): I = Image.open(dataDir/class_name/&#39;images&#39;/img[&#39;file_name&#39;].replace(&#39;png&#39;,&#39;jpg&#39;)) # Convert any grayscale or RBGA to RGB I = I.convert(&#39;RGB&#39;) og_h, og_w = I.size # Get Annotation of the Image annIds = coco.getAnnIds(imgIds=img[&#39;id&#39;], catIds=catIds, iscrowd=None) anns = coco.loadAnns(annIds) # Create Polygon for custom Annotation. og_poly = gen_original_poly(anns) # Get DETR output on our Image w.r.t COCO classes trans_img = transform(I).unsqueeze(0) out = model(trans_img.to(&#39;cuda&#39;)) # Create Masks by stacking Attention Maps and Pasting our Annotation # Excluding the functions definition for brevity. Can be found from colab link. class_masks = generate_class_maps(out) pred_mask, color2class = generate_pred_masks(class_masks, out[&#39;pred_masks&#39;].shape[2:]) pred_mask = cv2.resize(pred_mask, (og_h, og_w), interpolation= cv2.INTER_NEAREST) #Pasting Our Class on Mask for op in og_poly: cv2.fillPoly(pred_mask, pts = np.int32([op.get_xy()]), color = class_color) #Convering Mask to ID using panopticapi.utils mask_id = rgb2id(pred_mask) # Final Segmentation Details segments_info = generate_gt(mask_id, color2class, class_name) # The ID image(1 Channel) converted to 3 Channel Mask to save. img_save = Image.fromarray(id2rgb(mask_id)) mask_file_name = img[&#39;file_name&#39;].split(&#39;.&#39;)[0] + &#39;.png&#39; img_save.save(dataDir/class_name/&#39;annotations&#39;/mask_file_name) # Appending the Image Annotation to List class_annotaion.append( { &quot;segments_info&quot;: segments_info, &quot;file_name&quot;: mask_file_name, &quot;image_id&quot;: int(img[&#39;id&#39;]) } ) return class_annotaion, class_image . . for class_name in list_class: # Loop over all the classes names annFile = &lt;images_dir&gt;/class_name/&#39;coco.json&#39; # Path to the annotations file of each class coco = COCO(annFile) # Convert JSON to coco object (pycocotools.COCO) cats = coco.loadCats(coco.getCatIds()) # get all images containing given categories, select one at random catIds = coco.getCatIds(catNms=[class_name]); imgIds = coco.getImgIds(catIds=catIds); images = coco.loadImgs(imgIds) try: os.mkdir(&lt;images_dir&gt;/class_name/&#39;annotations&#39;) # Create Annotations Folder for each Class except FileExistsError as e: print(&#39;WARNING!&#39;, e) CLASS_ANNOTATION = run_class(images, class_name) # Generate Annotations for each class FINAL_JSON = {} FINAL_JSON[&#39;licenses&#39;] = coco.dataset[&#39;licenses&#39;] FINAL_JSON[&#39;info&#39;] = coco.dataset[&#39;info&#39;] FINAL_JSON[&#39;categories&#39;] = CATEGORIES FINAL_JSON[&#39;images&#39;] = coco.dataset[&#39;images&#39;] FINAL_JSON[&#39;annotations&#39;] = CLASS_ANNOTATION out_file = open(&lt;images_dir&gt;/class_name/&#39;annotations&#39;/f&#39;{class_name}.json&#39;, &quot;w&quot;) json.dump(FINAL_JSON, out_file, indent = 4) out_file.close() . . COCO Format . COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features with 80 object(things) classes, and 91 stuff classes for several tasks like captioning, segmentation, detection etc. . And it is the most widely used data as well as most widely used data format. So converting our annotations to the COCO format, would help us in levraging pre-built tools to create data pipelines to model. . File Structure . &lt;dataset_dir&gt;/ data/ &lt;filename0&gt;.&lt;ext&gt; &lt;filename1&gt;.&lt;ext&gt; ... annotations/ &lt;filename0&gt;.png &lt;filename1&gt;.png ... labels.json . The data folder has all the images, the images can be in image formats like JPG, JPEG, PNG. The annotations folder has the images of the masks, for every image in the data folder, with the same name and .png extension. The stem name of the file should match with the image in the data. The labels.json has 5 main keys, info, licenses, categories, images and annotations. This json file holds the data for the ground truth of the images. The format of the JSON can be varying as per the problem like object-detection, segmentation, keypoint-detection or image-captioning. . { &quot;info&quot;: info, &quot;licenses&quot;: [license], &quot;categories&quot;: [categories] &quot;images&quot;: [image], &quot;annotations&quot;: [annotation], } . The images, info and licenses remains same for all types, where as the annotations and categories format will differ. . info = { &quot;year&quot;: int, &quot;version&quot;: str, &quot;description&quot;: str, &quot;contributor&quot;: str, &quot;url&quot;: str, &quot;date_created&quot;: datetime, } image = { &quot;id&quot;: int, &quot;width&quot;: int, &quot;height&quot;: int, &quot;file_name&quot;: str, &quot;license&quot;: int, &quot;flickr_url&quot;: str, &quot;coco_url&quot;: str, &quot;date_captured&quot;: datetime, } license = { &quot;id&quot;: int, &quot;name&quot;: str, &quot;url&quot;: str, } . . Object Detection . Each object instance annotation contains a series of fields, including the category id and segmentation mask(optional if only Detection) of the object. An enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed). Finally, the categories field of the annotation structure stores the mapping of category id to category and supercategory names . annotation = { &quot;id&quot;: int, &quot;image_id&quot;: int, &quot;category_id&quot;: int, &quot;segmentation&quot;: RLE or [polygon], &quot;area&quot;: float, &quot;bbox&quot;: [x,y,width,height], &quot;iscrowd&quot;: 0 or 1, } categories = [{ &quot;id&quot;: int, &quot;name&quot;: str, &quot;supercategory&quot;: str, }] . . Panoptic Segmentation . For the panoptic task, each annotation struct is a per-image annotation rather than a per-object annotation. Each per-image annotation has two parts: (1) a PNG that stores the class-agnostic image segmentation and (2) a JSON struct that stores the semantic information for each image segment . To match an annotation with an image, use the image_id field (that is annotation.image_id==image.id). | For each annotation, per-pixel segment ids are stored as a single PNG as the same name as image. | Each segment (whether it&#39;s a stuff or thing segment) is assigned a unique id. | Unlabeled pixels (void) are assigned a value of 0. Note that when you load the PNG as an RGB image, you will need to compute the ids via ids=R+G*256+B*256^2. | In annotation file. The segment_info.id stores the unique id of the segment and is used to retrieve the corresponding mask from the PNG (ids==segment_info.id). | Finally, each category struct has two additional fields: isthing that distinguishes stuff and thing categories and color that is useful for consistent visualization. | . annotation = { &quot;image_id&quot;: int, &quot;file_name&quot;: str, &quot;segments_info&quot;: [segment_info], } segment_info = { &quot;id&quot;: int, &quot;category_id&quot;: int, &quot;area&quot;: int, &quot;bbox&quot;: [x,y,width,height], &quot;iscrowd&quot;: 0 or 1, } categories = [{ &quot;id&quot;: int, &quot;name&quot;: str, &quot;supercategory&quot;: str, &quot;isthing&quot;: 0 or 1, &quot;color&quot;: [R,G,B], }] . .",
            "url": "https://abdksyed.github.io/blog/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/09/30/CustomDataset.html",
            "relUrl": "/custom%20dataset/object%20detection/panoptic%20segmentation/coco/detr/2021/09/30/CustomDataset.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://abdksyed.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://abdksyed.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://abdksyed.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abdksyed.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}